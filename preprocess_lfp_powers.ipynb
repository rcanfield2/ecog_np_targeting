{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac050bf3-ff0c-4dd1-8fa1-d04f493b96a7",
   "metadata": {},
   "source": [
    "This script loads raw neuropixel spike times and prepares data for further analysis by performing the following steps:\n",
    "1. Select which task entries to analyze\n",
    "2. Load behavioral data and select good trials based on the reach time distributions.\n",
    "3. Load neuropixel spike times\n",
    "    - Bin spike times\n",
    "    - Align data to the even of interest\n",
    "    - Smooth timeseries with a Gaussian kernel\n",
    "4. Saves preprocessed data\n",
    "5. Plots basic neural data figures\n",
    "    - Trial averaged firing rate\n",
    "    - Raster plots\n",
    "    - Raster plots organized by target direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb7086-70d1-4e3b-9508-43151da8df12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:36:03.291137Z",
     "start_time": "2024-05-03T18:35:59.145410Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:20:04.961546Z",
     "iopub.status.busy": "2024-09-21T20:20:04.960972Z",
     "iopub.status.idle": "2024-09-21T20:20:08.625743Z",
     "shell.execute_reply": "2024-09-21T20:20:08.623705Z",
     "shell.execute_reply.started": "2024-09-21T20:20:04.961485Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import aopy\n",
    "import os\n",
    "import pandas as pds\n",
    "from db import dbfunctions as db\n",
    "from ipywidgets import interactive, widgets\n",
    "import scipy\n",
    "import h5py\n",
    "from tqdm.auto import tqdm \n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "from datetime import date\n",
    "from ibldsp.voltage import detect_bad_channels, interpolate_bad_channels, destripe_lfp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931fac6",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7cb844-4915-420c-984b-581e0c9685bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:36:03.415295Z",
     "start_time": "2024-05-03T18:36:03.294173Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:20:08.627930Z",
     "iopub.status.busy": "2024-09-21T20:20:08.627507Z",
     "iopub.status.idle": "2024-09-21T20:20:08.764625Z",
     "shell.execute_reply": "2024-09-21T20:20:08.763138Z",
     "shell.execute_reply.started": "2024-09-21T20:20:08.627901Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 2.0 3\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "subject = 'affi'\n",
    "data_path_preproc = '/media/moor-data/preprocessed.new/'\n",
    "data_path_raw = '/media/moor-data/raw/neuropixels/'\n",
    "save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/np_analysis_preproc_data\"\n",
    "behavior_save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/behavior\"\n",
    "lfp_power_save_dir = f\"/media/moor-data/postprocessed/{subject}/neuropixel_lfp_power\"\n",
    "\n",
    "base_save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/\"\n",
    "np_preproc_data_folder = 'np_analysis_preproc_data'\n",
    "ecog_dec_acc_file_name = 'ecog_decoding_maps/npinsert_ecog_decoding'\n",
    "save_lfp_power = True\n",
    "\n",
    "# General data parameters\n",
    "task_coords = 'yzx'\n",
    "task_perturb = None\n",
    "task_rotation = 0\n",
    "\n",
    "# Task event code definitions\n",
    "task_codes = aopy.data.bmi3d.load_bmi3d_task_codes()\n",
    "CENTER_TARGET_ON = 16\n",
    "CURSOR_ENTER_CENTER_TARGET = 80\n",
    "CURSOR_ENTER_PERIPHERAL_TARGET = list(range(81,89))\n",
    "PERIPHERAL_TARGET_ON = list(range(17,25))\n",
    "CENTER_TARGET_OFF = 32\n",
    "REWARD = 48\n",
    "DELAY_PENALTY = 66\n",
    "TIMEOUT_PENALTY = 65\n",
    "HOLD_PENALTY = 64\n",
    "PAUSE = 254\n",
    "TIME_ZERO = 238\n",
    "TRIAL_END = 239\n",
    "\n",
    "# Trial selection parameters\n",
    "trial_filter = lambda t: CENTER_TARGET_OFF in t\n",
    "success_rate_window = 19\n",
    "reach_time_std_thresh = 3\n",
    "\n",
    "# Neuropixel data parameters\n",
    "implant_name = ['NP_Insert72', 'NP_Insert137']\n",
    "start_date = '2023-07-13'\n",
    "if subject == 'beignet':\n",
    "    end_date = '2024-02-05' # for beignet\n",
    "else:\n",
    "    start_date = '2024-05-20'\n",
    "    end_date = date.today()\n",
    "elec_config = 'bottom'\n",
    "spike_bin_width_mc = 0.01 #[s]\n",
    "smooth_width = 150\n",
    "smooth_nstd = 3\n",
    "\n",
    "# Task data selection parameters\n",
    "tbefore_mc = 0.2\n",
    "tafter_mc = .8\n",
    "\n",
    "# Filter parameters\n",
    "bands = [(1,4), (4,12), (12,30), (30,80), (80,200)]\n",
    "n = 0.5\n",
    "w = 4\n",
    "n, p, k = aopy.precondition.base.convert_taper_parameters(n,w)\n",
    "print(n, p, k)\n",
    "\n",
    "# Visualization parameters\n",
    "colors = sn.color_palette(n_colors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68d1899-903a-4783-9967-8f99303b68b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:36:03.428058Z",
     "start_time": "2024-05-03T18:36:03.417715Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:20:08.766566Z",
     "iopub.status.busy": "2024-09-21T20:20:08.766199Z",
     "iopub.status.idle": "2024-09-21T20:20:08.779063Z",
     "shell.execute_reply": "2024-09-21T20:20:08.777352Z",
     "shell.execute_reply.started": "2024-09-21T20:20:08.766529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cursor_leave_center_time(data, samplerate, target_radius):\n",
    "    '''\n",
    "    Compute the time when the cursor leaves the center target radius\n",
    "    \n",
    "    Args:\n",
    "        traj (ntrials list of (nt,2)): x,y trajectory data\n",
    "        samplerate\n",
    "        target_radius (float): the radius of the center target\n",
    "        \n",
    "    Returns:\n",
    "        cursor_leave_center_time (ntrials list): the time when the cursor leaves the center target radius\n",
    "    '''\n",
    "    ntr = len(data)\n",
    "    cursor_leave_center_time = []\n",
    "    \n",
    "    for itr in range(ntr):\n",
    "        t_axis = np.arange(data[itr].shape[0])/samplerate\n",
    "        \n",
    "        dist = np.sqrt(data[itr][:,0]**2 + data[itr][:,1]**2)\n",
    "        leave_idx = np.where(dist>target_radius)[0]\n",
    "        temp = t_axis[leave_idx]\n",
    "        cursor_leave_center_time.append(temp[0])\n",
    "    \n",
    "    return cursor_leave_center_time\n",
    "\n",
    "def get_cursor_leave_center_idx(data, target_radius):\n",
    "    '''\n",
    "    Compute the time when the cursor leaves the center target radius\n",
    "    \n",
    "    Args:\n",
    "        traj (ntrials list of (nt,2)): x,y trajectory data\n",
    "        target_radius (float): the radius of the center target\n",
    "        \n",
    "    Returns:\n",
    "        cursor_leave_center_time (ntrials list): the time when the cursor leaves the center target radius. Nan if cursor doesn't leave center target.\n",
    "    '''\n",
    "    ntr = len(data)\n",
    "    cursor_leave_center_time = []\n",
    "    leave_idx = []\n",
    "    for itr in range(ntr):\n",
    "        dist = np.sqrt(data[itr][:,0]**2 + data[itr][:,1]**2)\n",
    "        \n",
    "        try:\n",
    "            temp_leave_idx = np.where(dist>target_radius)[0][0]\n",
    "        except:\n",
    "            temp_leave_idx = np.nan\n",
    "        leave_idx.append(temp_leave_idx)\n",
    "    \n",
    "    return leave_idx\n",
    "\n",
    "def smooth_timeseries_gaus(timeseries_data, samplerate, width, nstd=3, conv_mode='same'):\n",
    "    '''\n",
    "    Smooths across 2 \n",
    "    \n",
    "    Args:\n",
    "        timeseries_data (ntime, ...)\n",
    "        samplerate (int): Sample rate of timeseries\n",
    "        width (float): Width of the gaussian in time [ms] from -nstd to +nstd\n",
    "        nstd (float/int): Number of standard deviations to be used in the filter calculation.\n",
    "        conv_mode (str): Sets the size of the output. Takes eithe 'full', 'valid', or 'same'. See scipy.signal.convolve for full documentationat\n",
    "        \n",
    "    Returns: \n",
    "        smoothed_timeseries\n",
    "    '''\n",
    "    sample_std = (width/nstd)*(samplerate/(1000)) # Convert from s to ms\n",
    "    x = np.arange(-sample_std*nstd, nstd*sample_std+1)\n",
    "    gaus_filter = (1/(sample_std*np.sqrt(2*np.pi)))*np.exp(-(x**2)/(2*sample_std**2))\n",
    "    return np.apply_along_axis(scipy.signal.convolve, 0, timeseries_data, gaus_filter, mode=conv_mode, method='direct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277b839-a384-441f-b0c5-463988c44011",
   "metadata": {},
   "source": [
    "# Select relevant task entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c65d569-1f01-41ad-b204-307475974f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:36:03.599978Z",
     "start_time": "2024-05-03T18:36:03.436723Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:20:08.780800Z",
     "iopub.status.busy": "2024-09-21T20:20:08.780436Z",
     "iopub.status.idle": "2024-09-21T20:20:08.976464Z",
     "shell.execute_reply": "2024-09-21T20:20:08.974695Z",
     "shell.execute_reply.started": "2024-09-21T20:20:08.780762Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-30 10:16:10.257885: affi on manual control task, id=17536, 2024-05-31 09:46:01.684863: affi on manual control task, id=17542, 2024-05-31 10:19:02.740122: affi on manual control task, id=17543, 2024-06-02 09:37:43.847715: affi on manual control task, id=17553, 2024-06-03 08:49:00.861833: affi on manual control task, id=17556, 2024-06-04 09:12:52.159749: affi on manual control task, id=17560, 2024-06-05 09:28:39.226492: affi on manual control task, id=17568, 2024-06-06 09:40:49.027110: affi on manual control task, id=17571, 2024-06-07 09:06:58.232177: affi on manual control task, id=17574, 2024-08-28 09:36:13.293318: affi on manual control task, id=18110, 2024-08-29 09:59:03.643873: affi on manual control task, id=18128, 2024-08-30 09:13:04.119401: affi on manual control task, id=18136, 2024-09-02 08:45:27.849920: affi on manual control task, id=18166, 2024-09-03 09:44:33.503732: affi on manual control task, id=18170, 2024-09-04 10:05:24.581010: affi on manual control task, id=18189, 2024-09-05 10:20:25.194680: affi on manual control task, id=18192, 2024-09-06 09:11:38.246771: affi on manual control task, id=18197, 2024-09-07 09:34:09.260161: affi on manual control task, id=18199, 2024-09-09 09:19:59.751103: affi on manual control task, id=18205, 2024-09-10 10:09:40.418490: affi on manual control task, id=18223, 2024-09-11 09:31:11.835364: affi on manual control task, id=18229, 2024-09-12 10:01:09.483657: affi on manual control task, id=18236, 2024-09-14 08:57:25.197819: affi on manual control task, id=18272, 2024-09-15 08:52:39.677510: affi on manual control task, id=18274, 2024-09-17 08:44:41.983058: affi on manual control task, id=18291, 2024-09-18 11:46:11.651234: affi on manual control task, id=18320, 2024-09-19 10:42:59.527537: affi on manual control task, id=18335, 2024-09-20 09:49:21.737751: affi on manual control task, id=18345] \n",
      " \n",
      " [datetime.date(2024, 5, 30) datetime.date(2024, 5, 31)\n",
      " datetime.date(2024, 6, 2) datetime.date(2024, 6, 3)\n",
      " datetime.date(2024, 6, 4) datetime.date(2024, 6, 5)\n",
      " datetime.date(2024, 6, 6) datetime.date(2024, 6, 7)\n",
      " datetime.date(2024, 8, 28) datetime.date(2024, 8, 29)\n",
      " datetime.date(2024, 8, 30) datetime.date(2024, 9, 2)\n",
      " datetime.date(2024, 9, 3) datetime.date(2024, 9, 4)\n",
      " datetime.date(2024, 9, 5) datetime.date(2024, 9, 6)\n",
      " datetime.date(2024, 9, 7) datetime.date(2024, 9, 9)\n",
      " datetime.date(2024, 9, 10) datetime.date(2024, 9, 11)\n",
      " datetime.date(2024, 9, 12) datetime.date(2024, 9, 14)\n",
      " datetime.date(2024, 9, 15) datetime.date(2024, 9, 17)\n",
      " datetime.date(2024, 9, 18) datetime.date(2024, 9, 19)\n",
      " datetime.date(2024, 9, 20)]\n"
     ]
    }
   ],
   "source": [
    "# Load neuropixel center-out task data\n",
    "# Beignet\n",
    "if subject == 'beignet':\n",
    "    # bad_tes = [13152, 13153, 13154, 13155, 13156, 13102]\n",
    "    bad_tes = [13152, 13153, 13154, 13155, 13156, 13272, 12290, 12291, 13102, 11971] # Also remove recording at site 48 (12290 & 12291)\n",
    "    # bad_tes = [13102,13152, 13153, 13154, 13155, 13156, 13272, 9940, 9958, 10812, 10820, 12290, 12291]\n",
    "\n",
    "# Affi\n",
    "elif subject == 'affi':\n",
    "    bad_tes = [11971, 11974, 11981, 11982, 11999, 12001, 12013, 12016, 12027, 12028, 12385, 12389, 12390, 12391, 12392, 12393, 12394, 12396, 12397,\n",
    "              17294, 17296, 17297, 17299, 17301, 17302, 17303, 17304, 17305, 17316, 17318, 17319, 17547,17548, 17552, 17558, 12365, 12000, \n",
    "              18161, 18162, 18164, 18169, 18202, 18204,] \n",
    "\n",
    "mc_entries =  db.get_task_entries(subject__name=subject, task__name='manual control', date=(start_date, end_date))\n",
    "mc_entries = [me for me in mc_entries if 'neuropixel_port1_drive_type' in me.task_params and me.task_params['neuropixel_port1_drive_type'] in implant_name\n",
    "             and me.task_params['rotation']==task_coords and me.entry_name != 'flash']\n",
    "\n",
    "# Remove bad TE IDs\n",
    "mc_entries = [me for me in mc_entries if me.id not in bad_tes]\n",
    "dates = np.unique([me.date.date() for me in mc_entries])\n",
    "\n",
    "print(mc_entries, '\\n','\\n', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cd896-a30a-4388-b826-1e0d9d733e2c",
   "metadata": {},
   "source": [
    "# Load behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adc1c6f-4333-4998-950c-cb0e7792e29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T20:20:08.979087Z",
     "iopub.status.busy": "2024-09-21T20:20:08.978824Z",
     "iopub.status.idle": "2024-09-21T20:27:10.516415Z",
     "shell.execute_reply": "2024-09-21T20:27:10.514409Z",
     "shell.execute_reply.started": "2024-09-21T20:20:08.979066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0 min to load preprocessed data\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "aopy.utils.release_memory_limit()\n",
    "df, rasters, preproc_metadata = aopy.data.base.pkl_read(f\"{subject}_np_preprocessed\", os.path.join(base_save_dir, np_preproc_data_folder))\n",
    "print(f\"{np.round((time.time()-start)/60)} min to load preprocessed data\")\n",
    "nrecs = preproc_metadata['nrecs']\n",
    "recording_site = preproc_metadata['recording_sites'] # will be the same for all align events\n",
    "implants = ['NPinsert72' if preproc_metadata['implant'][irec] == 'NP_Insert72' else 'NPinsert137' for irec in range(len(preproc_metadata['implant']))] #Rename because name in bmi3d is slightly different (TODO)\n",
    "dates = np.unique(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf663d9c-1fd6-4cfb-8675-bce96960c3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:28:55.009490Z",
     "start_time": "2024-05-03T18:36:03.604742Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.517953Z",
     "iopub.status.busy": "2024-09-21T20:27:10.517755Z",
     "iopub.status.idle": "2024-09-21T20:27:10.524854Z",
     "shell.execute_reply": "2024-09-21T20:27:10.523371Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.517933Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subjects = [subject for me in mc_entries]\n",
    "# te_ids = [me.id for me in mc_entries]\n",
    "# me_dates = [me.date.date() for me in mc_entries]\n",
    "# df = aopy.data.bmi3d.tabulate_behavior_data_center_out(data_path_preproc, subjects, te_ids, me_dates, metadata=['target_radius'])\n",
    "# df['reward'] = df['reward'].astype(bool)\n",
    "# success_rate = aopy.analysis.calc_success_rate_trials(df['reward'], df['reach_completed'], window_size=success_rate_window)\n",
    "# success_rate_date_labels = df['date']\n",
    "# df = df[df['reward']].reset_index(drop=True)\n",
    "\n",
    "# # Add cursor trajectories\n",
    "# traj_times = np.array([(hst-tbefore_mc, e[-1]+tafter_mc) for hst, e in zip(df['hold_start_time'], df['event_times'])])\n",
    "# df['cursor_traj'] = aopy.data.bmi3d.tabulate_kinematic_data(data_path_preproc, df['subject'], df['te_id'], df['date'], traj_times[:,0], traj_times[:,1], datatype='cursor')\n",
    "# df['hand_traj'] = aopy.data.bmi3d.tabulate_kinematic_data(data_path_preproc, df['subject'], df['te_id'], df['date'], traj_times[:,0], traj_times[:,1], datatype='hand')\n",
    "# df['start_time'] = traj_times[:,0]\n",
    "\n",
    "# # Add behavior metrics\n",
    "# df['duration'] = [(t[-1]-t[0])- (tbefore_mc+tafter_mc) for t in traj_times] \n",
    "# cursor_traj = [np.array(t) for t in df['cursor_traj']]\n",
    "# hand_traj = [np.array(t) for t in df['hand_traj']]\n",
    "# df['cursor_vel_traj'] = [np.array([aopy.utils.derivative(np.arange(len(t))/1000, t[:,0]), aopy.utils.derivative(np.arange(len(t))/1000, t[:,1])]).T for t in cursor_traj]\n",
    "# df['cursor_vel'] =  [np.mean(aopy.utils.derivative(np.arange(len(t))/1000, t)) for t in cursor_traj]\n",
    "# df['hand_vel_traj'] = [np.array([aopy.utils.derivative(np.arange(len(t))/1000, t[:,0]), aopy.utils.derivative(np.arange(len(t))/1000, t[:,1]), aopy.utils.derivative(np.arange(len(t))/1000, t[:,2])]).T for t in hand_traj]\n",
    "# df['hand_vel'] =  [np.mean(aopy.utils.derivative(np.arange(len(t))/1000, t)) for t in hand_traj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71b386e-f1a4-4cc8-b5ae-f1cd5b24da45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:28:56.862544Z",
     "start_time": "2024-05-03T19:28:55.012122Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.525881Z",
     "iopub.status.busy": "2024-09-21T20:27:10.525722Z",
     "iopub.status.idle": "2024-09-21T20:27:10.531196Z",
     "shell.execute_reply": "2024-09-21T20:27:10.529719Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.525866Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get specs of loaded data\n",
    "# reach_times = np.array(df['duration'][df['reward']])\n",
    "# n_mctrials = [len(df[(df['date']==date)*df['reward']]) for date in dates]\n",
    "# ntargets = len(np.unique(np.array(df['target_idx'][df['reward']])))\n",
    "# unique_targets = aopy.data.bmi3d.get_target_locations(data_path_preproc, subject, df['te_id'][0], df['date'][0], np.unique(df['target_idx']))\n",
    "# reach_time_thresh = np.median(np.hstack(reach_times)) + (np.median(np.hstack(reach_times))-np.min(np.hstack(reach_times)))\n",
    "# good_trial_idx1 = df['duration'] <= reach_time_thresh # Labels for reach trials less than the max time (doesn't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783ba6e7-e014-4468-b4cd-8453b5db9d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:28:57.127133Z",
     "start_time": "2024-05-03T19:28:56.864933Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.532181Z",
     "iopub.status.busy": "2024-09-21T20:27:10.532020Z",
     "iopub.status.idle": "2024-09-21T20:27:10.538065Z",
     "shell.execute_reply": "2024-09-21T20:27:10.536665Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.532167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define 'good_trial_idx' so that all targets from all penetrations have the same number of trials\n",
    "# min_trials_to_target = np.min([np.min(np.unique(np.array(df['target_idx'])[df['date']==date][good_trial_idx1[df['date']==date]], return_counts=True)[1]) for dateidx, date in enumerate(dates)])\n",
    "# ngood_trials = ntargets*min_trials_to_target\n",
    "# df['good_trial'] = False\n",
    "# good_trial_idx = []\n",
    "# for idate, date in enumerate(dates):\n",
    "#     good_trial_idx_temp = []    \n",
    "#     [good_trial_idx_temp.extend(np.where(np.logical_and(df['target_idx'][df['date']==date]==itarget+1, good_trial_idx1[df['date']==date]))[0][:min_trials_to_target]) for itarget in range(ntargets)]\n",
    "#     good_trial_idx_mask = np.zeros(n_mctrials[idate], dtype=bool)\n",
    "#     good_trial_idx_mask[good_trial_idx_temp] = True\n",
    "#     # df['good_trial'][df['date']==date] = good_trial_idx_mask\n",
    "#     df.loc[df['date']==date, ['good_trial']] = good_trial_idx_mask\n",
    "#     # df.loc[df['te_id']==me.id, ['recording_site']] = exp_metadata['neuropixel_port1_site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1aa43c1-6d25-44ea-a755-0264d102c4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:29:30.364336Z",
     "start_time": "2024-05-03T19:28:59.162913Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.539144Z",
     "iopub.status.busy": "2024-09-21T20:27:10.538941Z",
     "iopub.status.idle": "2024-09-21T20:27:10.544496Z",
     "shell.execute_reply": "2024-09-21T20:27:10.543300Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.539125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # All trials\n",
    "# samplerate = 1000\n",
    "# go_cue_idx = np.ceil((df['go_cue_time'] - df['start_time'])*samplerate).astype(int)\n",
    "# trial_end_idx = np.ceil((df['reach_end_time'] - df['start_time'])*samplerate).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c786b813-feb4-4b7a-b619-b03f93aa66c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:29:51.670626Z",
     "start_time": "2024-05-03T19:29:51.486532Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.545778Z",
     "iopub.status.busy": "2024-09-21T20:27:10.545528Z",
     "iopub.status.idle": "2024-09-21T20:27:10.551665Z",
     "shell.execute_reply": "2024-09-21T20:27:10.550154Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.545755Z"
    }
   },
   "outputs": [],
   "source": [
    "# reach_time_list = [np.array(df['duration'][df['date']==date])[df['good_trial'][df['date']==date]] for date in dates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d590d9-722a-405a-85e7-0faaa944680f",
   "metadata": {},
   "source": [
    "# Load neuropixel spiking metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfa3c6c-bcb5-4138-9249-6acf36cd1a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:29:51.694477Z",
     "start_time": "2024-05-03T19:29:51.688849Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.553221Z",
     "iopub.status.busy": "2024-09-21T20:27:10.552954Z",
     "iopub.status.idle": "2024-09-21T20:27:10.558420Z",
     "shell.execute_reply": "2024-09-21T20:27:10.556945Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.553197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ntime = np.round((tafter_mc + tbefore_mc)/spike_bin_width_mc).astype(int)\n",
    "# trial_time_axis = np.arange(-tbefore_mc, tafter_mc, spike_bin_width_mc)\n",
    "# ntargets = len(np.unique(df['target_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9357a98b-cb84-48c1-9f82-4587d5c9d7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:59:58.284227Z",
     "start_time": "2024-05-03T19:29:51.696732Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.559929Z",
     "iopub.status.busy": "2024-09-21T20:27:10.559622Z",
     "iopub.status.idle": "2024-09-21T20:27:10.566478Z",
     "shell.execute_reply": "2024-09-21T20:27:10.565292Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.559901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Concatenate trials across sessions within a day\n",
    "# # TODO: concatenatmc_entries if recorded at the same stim site but during different days\n",
    "# start = time.time()\n",
    "# spike_times = []\n",
    "# unit_labels = []\n",
    "# trial_times = []\n",
    "# spike_segs = []\n",
    "# spike_align = []\n",
    "# spike_align_raster = []\n",
    "# spike_labels = []\n",
    "# spike_pos = []\n",
    "# ks_labels = []\n",
    "# recording_site = []\n",
    "# implant_name = []\n",
    "# df['recording_site'] = 0\n",
    "# df['implant_name'] = ''\n",
    "\n",
    "# for ime, me in enumerate(tqdm(mc_entries)):\n",
    "\n",
    "#     # Load data from sessions recorded on the same day and combine \n",
    "#     # Load data\n",
    "#     exp_data, exp_metadata = aopy.data.load_preproc_exp_data(data_path_preproc, subject, me.id, me.date.date())\n",
    "#     # filename_mc = aopy.data.get_preprocessed_filename(subject, me.id, me.date.date(), 'ap'\n",
    "        \n",
    "#     samplerate = exp_metadata['cursor_interp_samplerate']    \n",
    "    \n",
    "#     df.loc[df['te_id']==me.id, ['recording_site']] = exp_metadata['neuropixel_port1_site']\n",
    "#     df.loc[df['te_id']==me.id, ['implant_name']] = exp_metadata['neuropixel_port1_drive_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e888ac-d214-444b-9897-964c32a99eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T20:00:09.203197Z",
     "start_time": "2024-05-03T20:00:07.499899Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.568548Z",
     "iopub.status.busy": "2024-09-21T20:27:10.568098Z",
     "iopub.status.idle": "2024-09-21T20:27:10.576486Z",
     "shell.execute_reply": "2024-09-21T20:27:10.574973Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.568507Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get spike_seg idx for relevant events (Also will need to get idx for kinematics)\n",
    "# df['delay_start_kin_idx'] = np.ceil((df['delay_start_time'] - df['start_time'])*samplerate)\n",
    "# df['delay_start_neural_idx'] = np.ceil((df['delay_start_time'] - df['start_time'])*(1/spike_bin_width_mc))\n",
    "# df['go_cue_kin_idx'] = np.ceil((df['go_cue_time'] - df['start_time'])*samplerate)\n",
    "# df['go_cue_neural_idx'] = np.ceil((df['go_cue_time'] - df['start_time'])*(1/spike_bin_width_mc))\n",
    "# df['reach_end_kin_idx'] = np.ceil((df['reach_end_time'] - df['start_time'])*samplerate)\n",
    "# df['reach_end_neural_idx'] = np.ceil((df['reach_end_time'] - df['start_time'])*(1/spike_bin_width_mc))\n",
    "# df['mov_onset_kin_idx'] = 0\n",
    "# df['mov_onset_neural_idx'] = 0\n",
    "\n",
    "# # Calculate movement onset\n",
    "# for ite in np.unique(df['te_id']): # Each TE has the same target radius\n",
    "#     # Must start at delay_start_time \n",
    "#     traj = [temp_traj[np.array(go_cue_idx[df['te_id']==ite])[itraj]:,:] for itraj, temp_traj in enumerate(df['cursor_traj'][df['te_id']==ite])]\n",
    "#     df.loc[df['te_id']==ite, ['mov_onset_kin_idx']] = np.array(get_cursor_leave_center_idx(traj, np.array(df['target_radius'][df['te_id']==ite])[0])) + np.array(go_cue_idx[df['te_id']==ite])\n",
    "#     kin_neural_samplerate_ratio = samplerate/(1/spike_bin_width_mc)\n",
    "#     df.loc[df['te_id']==ite, ['mov_onset_neural_idx']] = np.array(df['mov_onset_kin_idx'][df['te_id']==ite])//kin_neural_samplerate_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50826ac-c1bc-40d2-beef-329d95400719",
   "metadata": {},
   "source": [
    "# Load neuropixel LFP power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df61627b-7298-4166-bd1c-3f78fc5d565f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T20:04:47.923354Z",
     "start_time": "2024-05-03T20:04:47.913292Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.582563Z",
     "iopub.status.busy": "2024-09-21T20:27:10.581592Z",
     "iopub.status.idle": "2024-09-21T20:27:10.590339Z",
     "shell.execute_reply": "2024-09-21T20:27:10.588810Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.582514Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['recording_site'][(df['recording_site']==55)*(df['date']==dates[2])] = 56\n",
    "# df['recording_site'][(df['recording_site']==55)*(df['date']==dates[4])] = 47\n",
    "# # ks_combined_tes = {dates[0]: [9922, 9924, 9925],     # (2023, 7, 13)\n",
    "# #                    dates[1]: [9928, 9929],           # (2023, 7, 14)\n",
    "# #                    dates[2]: [9940],                 # (2023, 7, 18)\n",
    "# #                    dates[3]: [9958],                 # (2023, 7, 19)\n",
    "# #                    dates[4]: [10799, 10800, 10802],  # (2023, 8, 28)\n",
    "# #                    dates[5]: [10810, 10812],         # (2023, 8, 29)\n",
    "# #                    dates[6]: [10818, 10820],         # (2023, 8, 30)\n",
    "# #                    dates[7]: [10824, 10827, 10828],  # (2023, 8, 31)\n",
    "# #                    dates[8]: [10835],                # (2023, 9, 1)\n",
    "# #                    dates[9]: [12269, 12270],         # (2023, 11, 16)\n",
    "# #                    dates[10]: [13122],               # (2023, 12, 28)\n",
    "# #                    dates[11]: [13239],               # (2023, 1, 3)\n",
    "# #                    dates[12]: [13256],               # (2023, 1, 4)\n",
    "# #                    dates[13]: [14116],               # (2023, 2, 1)\n",
    "# #                    dates[14]: [14139, 14141]}        # (2023, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0003ea5-3e37-41a2-a0ee-74cf0afe4b3d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-03T18:35:59.276Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T20:27:10.592262Z",
     "iopub.status.busy": "2024-09-21T20:27:10.591850Z",
     "iopub.status.idle": "2024-09-22T00:24:18.421682Z",
     "shell.execute_reply": "2024-09-22T00:24:18.419631Z",
     "shell.execute_reply.started": "2024-09-21T20:27:10.592224Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d5f65355ec43a4a2b33e1d2846c3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFP power data already exists from te11983\n",
      "LFP power already exists for all TEs on 2023-10-31\n",
      "LFP power data already exists from te12383\n",
      "LFP power data already exists from te12386\n",
      "LFP power already exists for all TEs on 2023-11-28\n",
      "LFP power data already exists from te17536\n",
      "LFP power already exists for all TEs on 2024-05-30\n",
      "LFP power data already exists from te17542\n",
      "LFP power data already exists from te17543\n",
      "LFP power already exists for all TEs on 2024-05-31\n",
      "LFP power data already exists from te17553\n",
      "LFP power already exists for all TEs on 2024-06-02\n",
      "LFP power data already exists from te17556\n",
      "LFP power already exists for all TEs on 2024-06-03\n",
      "LFP power data already exists from te17560\n",
      "LFP power already exists for all TEs on 2024-06-04\n",
      "LFP power data already exists from te17568\n",
      "LFP power already exists for all TEs on 2024-06-05\n",
      "LFP power data already exists from te17571\n",
      "LFP power already exists for all TEs on 2024-06-06\n",
      "LFP power data already exists from te17574\n",
      "LFP power already exists for all TEs on 2024-06-07\n",
      "LFP power data already exists from te18110\n",
      "LFP power already exists for all TEs on 2024-08-28\n",
      "LFP power data already exists from te18128\n",
      "LFP power already exists for all TEs on 2024-08-29\n",
      "LFP power data already exists from te18136\n",
      "LFP power already exists for all TEs on 2024-08-30\n",
      "LFP power data already exists from te18166\n",
      "LFP power already exists for all TEs on 2024-09-02\n",
      "LFP power data already exists from te18170\n",
      "LFP power already exists for all TEs on 2024-09-03\n",
      "LFP power data already exists from te18189\n",
      "LFP power already exists for all TEs on 2024-09-04\n",
      "LFP power data already exists from te18192\n",
      "LFP power already exists for all TEs on 2024-09-05\n",
      "LFP power data already exists from te18197\n",
      "LFP power already exists for all TEs on 2024-09-06\n",
      "LFP power data already exists from te18199\n",
      "LFP power already exists for all TEs on 2024-09-07\n",
      "LFP power data already exists from te18205\n",
      "LFP power already exists for all TEs on 2024-09-09\n",
      "LFP power data already exists from te18223\n",
      "LFP power already exists for all TEs on 2024-09-10\n",
      "LFP power data already exists from te18229\n",
      "LFP power already exists for all TEs on 2024-09-11\n",
      "LFP power data already exists from te18236\n",
      "LFP power already exists for all TEs on 2024-09-12\n",
      "LFP power data already exists from te18272\n",
      "LFP power already exists for all TEs on 2024-09-14\n",
      "LFP power data already exists from te18274\n",
      "LFP power already exists for all TEs on 2024-09-15\n",
      "No LFP power data found for te18291 - processing now....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5847308cc2845289b9a46338d8c7283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LFP power data for te18291\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "# dtype = 'int16'\n",
    "nch = 384\n",
    "ks_folder_name_cutoff2 = date(2024, 8, 15)\n",
    "\n",
    "aopy.utils.release_memory_limit()\n",
    "lfp_spec_segs = []\n",
    "for idate, date in enumerate(tqdm(dates[:])):\n",
    "    # if date > ks_folder_name_cutoff:\n",
    "    #     ks_preproc_path = os.path.join(data_path_preproc, f\"kilosort/{date}_Neuropixel_ks_{subject}_site{list(df['recording_site'][df['date']==date])[0]}_bottom_port1\")\n",
    "    # else:\n",
    "    #     ks_preproc_path = os.path.join(data_path_preproc, f\"kilosort/{date}_Neuropixel_ks_{subject}_bottom_port1\")\n",
    "    \n",
    "    # If all data is loaded from this day, go to next day\n",
    "    nte_id = np.unique(df['te_id'][df['date']==date])\n",
    "    try:\n",
    "        for ite_id, te_id_temp in enumerate(nte_id):\n",
    "            _ = aopy.data.base.pkl_read(f'lfp_power_te{te_id_temp}', lfp_power_save_dir)\n",
    "            # apband_spec_segs.extend(apband_spec_segs_teid)\n",
    "            print(f\"LFP power data already exists from te{te_id_temp}\")\n",
    "\n",
    "        print(f\"LFP power already exists for all TEs on {date}\")\n",
    "    except:\n",
    "        # # Get whitening matrix\n",
    "        # rez_path = os.path.join(ks_preproc_path, 'kilosort_output/rez.mat')\n",
    "        # with h5py.File(rez_path, 'r') as f:\n",
    "        #     whitening_matrix = np.array(f['rez']['Wrot'])\n",
    "\n",
    "        # inv_wht_matrix = np.linalg.pinv(whitening_matrix)\n",
    "\n",
    "        # # Load drift corrected and whitened apdata\n",
    "        # drift_corrected_apdata_path = os.path.join(ks_preproc_path, 'temp_wh.dat')\n",
    "        # data = np.memmap(drift_corrected_apdata_path, mode='r', dtype=dtype).reshape(-1,nch)\n",
    "\n",
    "        # # Correct by multiplying ap data by inverse whitening matrix\n",
    "        # corrected_data = data @ inv_wht_matrix\n",
    "        # # corrected_data = data\n",
    "\n",
    "        \n",
    "        # Load raw apdata and check that it is the same length. If not, subselect the relevant data. This happens because data was concatenated before kilosort    \n",
    "        # temp_data = []\n",
    "        # if len(ks_combined_tes[date]) > 1:\n",
    "        #     relevant_tes = np.unique(df['te_id'][df['date']==date])\n",
    "        #     start_sample = 0\n",
    "        #     for te_id_temp in ks_combined_tes[date]:\n",
    "        #         data_folder_mc = f\"{date}_Neuropixel_{subject}_te{te_id_temp}\"\n",
    "        #         rawdata_mc, rawmetadata = aopy.data.neuropixel.load_neuropixel_data(data_path_raw, data_folder_mc, 'ap')\n",
    "        #         end_sample = start_sample + rawdata_mc.samples.shape[0]\n",
    "        #         if te_id_temp in relevant_tes:\n",
    "        #             temp_data.append(corrected_data[start_sample:end_sample,:])\n",
    "        #         start_sample = end_sample\n",
    "\n",
    "        # else:\n",
    "        #     temp_data = [corrected_data]   \n",
    "        # del corrected_data, data # Clear up memory\n",
    "            # print(f\"WARNING: Mismatch of samples between temp_wh.dat file and raw ap data -- please check\")\n",
    "\n",
    "\n",
    "    #     # Downsample AP band data --- this needs to handle multiple TEs\n",
    "        for ite_id, te_id_temp in enumerate(nte_id):\n",
    "            try:\n",
    "                lfp_spec_segs_teid = aopy.data.base.pkl_read(f'lfp_power_te{te_id_temp}', lfp_power_save_dir)\n",
    "                # apband_spec_segs.extend(apband_spec_segs_teid)\n",
    "                print(f\"LFP power data already exists from te{te_id_temp}\")\n",
    "            except:\n",
    "                print(f\"No LFP power data found for te{te_id_temp} - processing now....\")\n",
    "                preproc_filename_mc = aopy.data.get_preprocessed_filename(subject, te_id_temp, date, 'lfp')\n",
    "                if date > ks_folder_name_cutoff2:\n",
    "                    preproc_filename_mc = preproc_filename_mc[:-4] + '_port1.hdf'\n",
    "                lfp_data_mc = aopy.data.load_hdf_group(os.path.join(data_path_preproc, subject), preproc_filename_mc, 'lfp')\n",
    "                lfp_metadata_mc = aopy.data.load_hdf_group(os.path.join(data_path_preproc, subject), preproc_filename_mc, 'metadata')\n",
    "                \n",
    "                clean_lfp_data = (destripe_lfp((lfp_data_mc['lfp']*lfp_metadata_mc['bit_volts']).T/1e6, lfp_metadata_mc['sample_rate'], neuropixel_version=1, channel_labels=True)*1e6).T # convert back to uV\n",
    "\n",
    "                # data_folder_mc = f\"{date}_Neuropixel_{subject}_te{te_id_temp}\"\n",
    "                # rawdata_mc, metadata_mc = aopy.data.neuropixel.load_neuropixel_data(data_path_raw, data_folder_mc, 'ap')\n",
    "                # corrected_apdata_dwns = aopy.precondition.base.downsample(temp_data[ite_id], metadata_mc['sample_rate'], samplerate_dwns)\n",
    "                # ap_timestamps,_ = aopy.preproc.base.interp_timestamps2timeseries(lfp_data_mc['sync_timestamp'], lfp_data_mc['sync_timestamp'], samplerate=samplerate_dwns) # upsample lfp timestamps to AP\n",
    "\n",
    "                # Trial align ap_timestamps for each TE and put into DF\n",
    "                traj_times = np.array([(hst-tbefore_mc, e[-1]+tafter_mc) for hst, e in zip(np.array(df['hold_start_time'][df['te_id']==te_id_temp]), np.array(df['event_times'][df['te_id']==te_id_temp]))])\n",
    "                ntrials_teid=traj_times.shape[0]\n",
    "                talign_times_mc_segs = []\n",
    "                talign_idx_mc_segs = []\n",
    "                lfp_spec_segs_teid = []\n",
    "                time_axes = []\n",
    "                for itrial in tqdm(range(ntrials_teid)):\n",
    "                    talign_times_mc, talign_idx_mc = aopy.preproc.base.trial_align_times(lfp_data_mc['sync_timestamp'], [traj_times[itrial,0]], 0, traj_times[itrial,1]-traj_times[itrial,0])\n",
    "                    talign_times_mc_segs.append(talign_times_mc)\n",
    "                    talign_idx_mc_segs.append(talign_idx_mc)\n",
    "                \n",
    "                    #filter\n",
    "                    t, spec_temp = aopy.analysis.base.get_bandpower_feats(clean_lfp_data[talign_idx_mc[0],:], lfp_metadata_mc['sample_rate'], bands=bands, log=True, ref=False, \n",
    "                                                                n=n, p=p, k=k, fk=bands[-1][1], step = spike_bin_width_mc)\n",
    "                    lfp_spec_segs_teid.append(spec_temp)\n",
    "                    time_axes.append(t-tbefore_mc)\n",
    "                \n",
    "                lfp_power_metadata = {'samplerate': int(1/spike_bin_width_mc), 'frequency_band': bands, 'ch_ypos': lfp_metadata_mc['ypos'], 'ch_xpos': lfp_metadata_mc['xpos'], 'time_axis':time_axes}\n",
    "\n",
    "                if save_lfp_power:\n",
    "                    print(f\"Saving LFP power data for te{te_id_temp}\")\n",
    "                    aopy.data.base.pkl_write(f'lfp_power_te{te_id_temp}', (lfp_spec_segs_teid, lfp_power_metadata), lfp_power_save_dir)\n",
    "                    print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3012ae-2b69-4570-97f7-6be2f21b7cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:np_targeting]",
   "language": "python",
   "name": "conda-env-np_targeting-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
