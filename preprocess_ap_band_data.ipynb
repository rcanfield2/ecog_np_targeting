{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac050bf3-ff0c-4dd1-8fa1-d04f493b96a7",
   "metadata": {},
   "source": [
    "This script loads raw neuropixel spike times and prepares data for further analysis by performing the following steps:\n",
    "1. Select which task entries to analyze\n",
    "2. Load behavioral data and select good trials based on the reach time distributions.\n",
    "3. Load neuropixel spike times\n",
    "    - Bin spike times\n",
    "    - Align data to the even of interest\n",
    "    - Smooth timeseries with a Gaussian kernel\n",
    "4. Saves preprocessed data\n",
    "5. Plots basic neural data figures\n",
    "    - Trial averaged firing rate\n",
    "    - Raster plots\n",
    "    - Raster plots organized by target direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb7086-70d1-4e3b-9508-43151da8df12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T19:03:41.011906Z",
     "start_time": "2024-08-01T19:03:41.007029Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:38:04.965746Z",
     "iopub.status.busy": "2024-09-19T05:38:04.965273Z",
     "iopub.status.idle": "2024-09-19T05:38:08.325548Z",
     "shell.execute_reply": "2024-09-19T05:38:08.324568Z",
     "shell.execute_reply.started": "2024-09-19T05:38:04.965693Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import aopy\n",
    "import os\n",
    "import pandas as pds\n",
    "from db import dbfunctions as db\n",
    "from ipywidgets import interactive, widgets\n",
    "import scipy\n",
    "import h5py\n",
    "from tqdm.auto import tqdm \n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931fac6",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7cb844-4915-420c-984b-581e0c9685bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T19:03:41.028713Z",
     "start_time": "2024-08-01T19:03:41.013714Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:38:08.327633Z",
     "iopub.status.busy": "2024-09-19T05:38:08.327262Z",
     "iopub.status.idle": "2024-09-19T05:38:08.339565Z",
     "shell.execute_reply": "2024-09-19T05:38:08.338658Z",
     "shell.execute_reply.started": "2024-09-19T05:38:08.327613Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "subject = 'affi'\n",
    "data_path_preproc = '/media/moor-data/preprocessed.new/'\n",
    "data_path_raw = '/media/moor-data/raw/neuropixels/'\n",
    "save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/np_analysis_preproc_data\"\n",
    "behavior_save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/behavior\"\n",
    "ap_band_power_save_dir = f\"/media/moor-data/postprocessed/{subject}/neuropixel_ap_band_power\"\n",
    "save_ap_band_power = True\n",
    "\n",
    "# General data parameters\n",
    "task_coords = 'yzx'\n",
    "task_perturb = None\n",
    "task_rotation = 0\n",
    "\n",
    "# Task event code definitions\n",
    "task_codes = aopy.data.bmi3d.load_bmi3d_task_codes()\n",
    "CENTER_TARGET_ON = 16\n",
    "CURSOR_ENTER_CENTER_TARGET = 80\n",
    "CURSOR_ENTER_PERIPHERAL_TARGET = list(range(81,89))\n",
    "PERIPHERAL_TARGET_ON = list(range(17,25))\n",
    "CENTER_TARGET_OFF = 32\n",
    "REWARD = 48\n",
    "DELAY_PENALTY = 66\n",
    "TIMEOUT_PENALTY = 65\n",
    "HOLD_PENALTY = 64\n",
    "PAUSE = 254\n",
    "TIME_ZERO = 238\n",
    "TRIAL_END = 239\n",
    "\n",
    "# # Select which event to align to\n",
    "# # align_event = 'TARGET ONSET'\n",
    "# # align_event = 'GO CUE'\n",
    "# align_event = 'MOVEMENT ONSET'\n",
    "# # align_event = 'ENTER TARGET'\n",
    "# END_TRIAL_CODE = REWARD\n",
    "\n",
    "\n",
    "# Trial selection parameters\n",
    "trial_filter = lambda t: CENTER_TARGET_OFF in t\n",
    "success_rate_window = 19\n",
    "reach_time_std_thresh = 3\n",
    "\n",
    "# Neuropixel data parameters\n",
    "implant_name = ['NP_Insert72', 'NP_Insert137']\n",
    "start_date = '2023-07-13'\n",
    "if subject == 'beignet':\n",
    "    end_date = '2024-02-05' # for beignet\n",
    "else:\n",
    "    start_date = '2023-11-01'\n",
    "    end_date = date.today()\n",
    "elec_config = 'bottom'\n",
    "spike_bin_width_mc = 0.01 #[s]\n",
    "smooth_width = 150\n",
    "smooth_nstd = 3\n",
    "\n",
    "# Task data selection parameters\n",
    "tbefore_mc = 0.2\n",
    "tafter_mc = .8\n",
    "\n",
    "# Visualization parameters\n",
    "colors = sn.color_palette(n_colors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68d1899-903a-4783-9967-8f99303b68b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T19:03:41.042767Z",
     "start_time": "2024-08-01T19:03:41.031832Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:38:08.340629Z",
     "iopub.status.busy": "2024-09-19T05:38:08.340441Z",
     "iopub.status.idle": "2024-09-19T05:38:08.350922Z",
     "shell.execute_reply": "2024-09-19T05:38:08.350066Z",
     "shell.execute_reply.started": "2024-09-19T05:38:08.340612Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cursor_leave_center_time(data, samplerate, target_radius):\n",
    "    '''\n",
    "    Compute the time when the cursor leaves the center target radius\n",
    "    \n",
    "    Args:\n",
    "        traj (ntrials list of (nt,2)): x,y trajectory data\n",
    "        samplerate\n",
    "        target_radius (float): the radius of the center target\n",
    "        \n",
    "    Returns:\n",
    "        cursor_leave_center_time (ntrials list): the time when the cursor leaves the center target radius\n",
    "    '''\n",
    "    ntr = len(data)\n",
    "    cursor_leave_center_time = []\n",
    "    \n",
    "    for itr in range(ntr):\n",
    "        t_axis = np.arange(data[itr].shape[0])/samplerate\n",
    "        \n",
    "        dist = np.sqrt(data[itr][:,0]**2 + data[itr][:,1]**2)\n",
    "        leave_idx = np.where(dist>target_radius)[0]\n",
    "        temp = t_axis[leave_idx]\n",
    "        cursor_leave_center_time.append(temp[0])\n",
    "    \n",
    "    return cursor_leave_center_time\n",
    "\n",
    "def get_cursor_leave_center_idx(data, target_radius):\n",
    "    '''\n",
    "    Compute the time when the cursor leaves the center target radius\n",
    "    \n",
    "    Args:\n",
    "        traj (ntrials list of (nt,2)): x,y trajectory data\n",
    "        target_radius (float): the radius of the center target\n",
    "        \n",
    "    Returns:\n",
    "        cursor_leave_center_time (ntrials list): the time when the cursor leaves the center target radius. Nan if cursor doesn't leave center target.\n",
    "    '''\n",
    "    ntr = len(data)\n",
    "    cursor_leave_center_time = []\n",
    "    leave_idx = []\n",
    "    for itr in range(ntr):\n",
    "        dist = np.sqrt(data[itr][:,0]**2 + data[itr][:,1]**2)\n",
    "        \n",
    "        try:\n",
    "            temp_leave_idx = np.where(dist>target_radius)[0][0]\n",
    "        except:\n",
    "            temp_leave_idx = np.nan\n",
    "        leave_idx.append(temp_leave_idx)\n",
    "    \n",
    "    return leave_idx\n",
    "\n",
    "def smooth_timeseries_gaus(timeseries_data, samplerate, width, nstd=3, conv_mode='same'):\n",
    "    '''\n",
    "    Smooths across 2 \n",
    "    \n",
    "    Args:\n",
    "        timeseries_data (ntime, ...)\n",
    "        samplerate (int): Sample rate of timeseries\n",
    "        width (float): Width of the gaussian in time [ms] from -nstd to +nstd\n",
    "        nstd (float/int): Number of standard deviations to be used in the filter calculation.\n",
    "        conv_mode (str): Sets the size of the output. Takes eithe 'full', 'valid', or 'same'. See scipy.signal.convolve for full documentationat\n",
    "        \n",
    "    Returns: \n",
    "        smoothed_timeseries\n",
    "    '''\n",
    "    sample_std = (width/nstd)*(samplerate/(1000)) # Convert from s to ms\n",
    "    x = np.arange(-sample_std*nstd, nstd*sample_std+1)\n",
    "    gaus_filter = (1/(sample_std*np.sqrt(2*np.pi)))*np.exp(-(x**2)/(2*sample_std**2))\n",
    "    return np.apply_along_axis(scipy.signal.convolve, 0, timeseries_data, gaus_filter, mode=conv_mode, method='direct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277b839-a384-441f-b0c5-463988c44011",
   "metadata": {},
   "source": [
    "# Select relevant task entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c65d569-1f01-41ad-b204-307475974f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T19:03:41.118515Z",
     "start_time": "2024-08-01T19:03:41.045135Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:38:08.351862Z",
     "iopub.status.busy": "2024-09-19T05:38:08.351670Z",
     "iopub.status.idle": "2024-09-19T05:38:08.669799Z",
     "shell.execute_reply": "2024-09-19T05:38:08.668368Z",
     "shell.execute_reply.started": "2024-09-19T05:38:08.351845Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-28 10:03:41.327000: affi on manual control task, id=12383, 2023-11-28 10:41:34.784000: affi on manual control task, id=12386, 2024-05-30 10:16:10.257885: affi on manual control task, id=17536, 2024-05-31 09:46:01.684863: affi on manual control task, id=17542, 2024-05-31 10:19:02.740122: affi on manual control task, id=17543, 2024-06-02 09:37:43.847715: affi on manual control task, id=17553, 2024-06-03 08:49:00.861833: affi on manual control task, id=17556, 2024-06-04 09:12:52.159749: affi on manual control task, id=17560, 2024-06-05 09:28:39.226492: affi on manual control task, id=17568, 2024-06-06 09:40:49.027110: affi on manual control task, id=17571, 2024-06-07 09:06:58.232177: affi on manual control task, id=17574, 2024-08-28 09:36:13.293318: affi on manual control task, id=18110, 2024-08-29 09:59:03.643873: affi on manual control task, id=18128, 2024-08-30 09:13:04.119401: affi on manual control task, id=18136, 2024-09-02 08:45:27.849920: affi on manual control task, id=18166, 2024-09-03 09:44:33.503732: affi on manual control task, id=18170, 2024-09-04 10:05:24.581010: affi on manual control task, id=18189, 2024-09-05 10:20:25.194680: affi on manual control task, id=18192, 2024-09-06 09:11:38.246771: affi on manual control task, id=18197, 2024-09-07 09:34:09.260161: affi on manual control task, id=18199, 2024-09-09 09:19:59.751103: affi on manual control task, id=18205, 2024-09-10 10:09:40.418490: affi on manual control task, id=18223, 2024-09-11 09:31:11.835364: affi on manual control task, id=18229, 2024-09-12 10:01:09.483657: affi on manual control task, id=18236, 2024-09-14 08:57:25.197819: affi on manual control task, id=18272, 2024-09-15 08:52:39.677510: affi on manual control task, id=18274, 2024-09-17 08:44:41.983058: affi on manual control task, id=18291] \n",
      " \n",
      " [datetime.date(2023, 11, 28) datetime.date(2024, 5, 30)\n",
      " datetime.date(2024, 5, 31) datetime.date(2024, 6, 2)\n",
      " datetime.date(2024, 6, 3) datetime.date(2024, 6, 4)\n",
      " datetime.date(2024, 6, 5) datetime.date(2024, 6, 6)\n",
      " datetime.date(2024, 6, 7) datetime.date(2024, 8, 28)\n",
      " datetime.date(2024, 8, 29) datetime.date(2024, 8, 30)\n",
      " datetime.date(2024, 9, 2) datetime.date(2024, 9, 3)\n",
      " datetime.date(2024, 9, 4) datetime.date(2024, 9, 5)\n",
      " datetime.date(2024, 9, 6) datetime.date(2024, 9, 7)\n",
      " datetime.date(2024, 9, 9) datetime.date(2024, 9, 10)\n",
      " datetime.date(2024, 9, 11) datetime.date(2024, 9, 12)\n",
      " datetime.date(2024, 9, 14) datetime.date(2024, 9, 15)\n",
      " datetime.date(2024, 9, 17)]\n"
     ]
    }
   ],
   "source": [
    "# Beignet\n",
    "if subject == 'beignet':\n",
    "    # bad_tes = [13152, 13153, 13154, 13155, 13156, 13102]\n",
    "    bad_tes = [13152, 13153, 13154, 13155, 13156, 13272, 12290, 12291, 13102, 11971] # Also remove recording at site 48 (12290 & 12291)\n",
    "    # bad_tes = [13102,13152, 13153, 13154, 13155, 13156, 13272, 9940, 9958, 10812, 10820, 12290, 12291]\n",
    "\n",
    "# Affi\n",
    "elif subject == 'affi':\n",
    "    bad_tes = [11971, 11974, 11981, 11982, 11999, 12001, 12013, 12016, 12027, 12028, 12385, 12389, 12390, 12391, 12392, 12393, 12394, 12396, 12397,\n",
    "              17294, 17296, 17297, 17299, 17301, 17302, 17303, 17304, 17305, 17316, 17318, 17319, 17547,17548, 17552, 17558, 12365, 12000, \n",
    "              18161, 18162, 18164, 18169,18202, 18204,\n",
    "              ]  # Days not fully preprocessed \n",
    "mc_entries =  db.get_task_entries(subject__name=subject, task__name='manual control', date=(start_date, end_date))\n",
    "mc_entries = [me for me in mc_entries if 'neuropixel_port1_drive_type' in me.task_params and me.task_params['neuropixel_port1_drive_type'] in implant_name\n",
    "             and me.task_params['rotation']==task_coords and me.entry_name != 'flash']\n",
    "\n",
    "# Remove bad TE IDs\n",
    "mc_entries = [me for me in mc_entries if me.id not in bad_tes]\n",
    "dates = np.unique([me.date.date() for me in mc_entries])\n",
    "\n",
    "print(mc_entries, '\\n','\\n', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cd896-a30a-4388-b826-1e0d9d733e2c",
   "metadata": {},
   "source": [
    "# Load behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf663d9c-1fd6-4cfb-8675-bce96960c3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:51:55.517233Z",
     "start_time": "2024-08-01T18:48:44.401376Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:38:08.672326Z",
     "iopub.status.busy": "2024-09-19T05:38:08.671635Z",
     "iopub.status.idle": "2024-09-19T05:51:13.564673Z",
     "shell.execute_reply": "2024-09-19T05:51:13.563625Z",
     "shell.execute_reply.started": "2024-09-19T05:38:08.672276Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8a5dc61b4f4278b5a3a1785c93a836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subjects = [subject for me in mc_entries]\n",
    "te_ids = [me.id for me in mc_entries]\n",
    "me_dates = [me.date.date() for me in mc_entries]\n",
    "df = aopy.data.bmi3d.tabulate_behavior_data_center_out(data_path_preproc, subjects, te_ids, me_dates, metadata=['target_radius'])\n",
    "df['reward'] = df['reward'].astype(bool)\n",
    "success_rate = aopy.analysis.calc_success_rate_trials(df['reward'], df['reach_completed'], window_size=success_rate_window)\n",
    "success_rate_date_labels = df['date']\n",
    "df = df[df['reward']].reset_index(drop=True)\n",
    "\n",
    "# Add cursor trajectories\n",
    "traj_times = np.array([(hst-tbefore_mc, e[-1]+tafter_mc) for hst, e in zip(df['hold_start_time'], df['event_times'])])\n",
    "df['cursor_traj'] = aopy.data.bmi3d.tabulate_kinematic_data(data_path_preproc, df['subject'], df['te_id'], df['date'], traj_times[:,0], traj_times[:,1], datatype='cursor')\n",
    "df['hand_traj'] = aopy.data.bmi3d.tabulate_kinematic_data(data_path_preproc, df['subject'], df['te_id'], df['date'], traj_times[:,0], traj_times[:,1], datatype='hand')\n",
    "df['start_time'] = traj_times[:,0]\n",
    "\n",
    "# Add behavior metrics\n",
    "df['duration'] = [(t[-1]-t[0])- (tbefore_mc+tafter_mc) for t in traj_times] \n",
    "cursor_traj = [np.array(t) for t in df['cursor_traj']]\n",
    "hand_traj = [np.array(t) for t in df['hand_traj']]\n",
    "df['cursor_vel_traj'] = [np.array([aopy.utils.derivative(np.arange(len(t))/1000, t[:,0]), aopy.utils.derivative(np.arange(len(t))/1000, t[:,1])]).T for t in cursor_traj]\n",
    "df['cursor_vel'] =  [np.mean(aopy.utils.derivative(np.arange(len(t))/1000, t)) for t in cursor_traj]\n",
    "df['hand_vel_traj'] = [np.array([aopy.utils.derivative(np.arange(len(t))/1000, t[:,0]), aopy.utils.derivative(np.arange(len(t))/1000, t[:,1]), aopy.utils.derivative(np.arange(len(t))/1000, t[:,2])]).T for t in hand_traj]\n",
    "df['hand_vel'] =  [np.mean(aopy.utils.derivative(np.arange(len(t))/1000, t)) for t in hand_traj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71b386e-f1a4-4cc8-b5ae-f1cd5b24da45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:51:58.183600Z",
     "start_time": "2024-08-01T18:51:55.519594Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:51:13.565890Z",
     "iopub.status.busy": "2024-09-19T05:51:13.565721Z",
     "iopub.status.idle": "2024-09-19T05:51:20.952034Z",
     "shell.execute_reply": "2024-09-19T05:51:20.950762Z",
     "shell.execute_reply.started": "2024-09-19T05:51:13.565876Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get specs of loaded data\n",
    "reach_times = np.array(df['duration'][df['reward']])\n",
    "n_mctrials = [len(df[(df['date']==date)*df['reward']]) for date in dates]\n",
    "ntargets = len(np.unique(np.array(df['target_idx'][df['reward']])))\n",
    "unique_targets = aopy.data.bmi3d.get_target_locations(data_path_preproc, subject, df['te_id'][0], df['date'][0], np.unique(df['target_idx']))\n",
    "reach_time_thresh = np.median(np.hstack(reach_times)) + (np.median(np.hstack(reach_times))-np.min(np.hstack(reach_times)))\n",
    "good_trial_idx1 = df['duration'] <= reach_time_thresh # Labels for reach trials less than the max time (doesn't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783ba6e7-e014-4468-b4cd-8453b5db9d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:51:58.367106Z",
     "start_time": "2024-08-01T18:51:58.186565Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:51:20.954793Z",
     "iopub.status.busy": "2024-09-19T05:51:20.954576Z",
     "iopub.status.idle": "2024-09-19T05:51:21.523781Z",
     "shell.execute_reply": "2024-09-19T05:51:21.522577Z",
     "shell.execute_reply.started": "2024-09-19T05:51:20.954774Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define 'good_trial_idx' so that all targets from all penetrations have the same number of trials\n",
    "min_trials_to_target = np.min([np.min(np.unique(np.array(df['target_idx'])[df['date']==date][good_trial_idx1[df['date']==date]], return_counts=True)[1]) for dateidx, date in enumerate(dates)])\n",
    "ngood_trials = ntargets*min_trials_to_target\n",
    "df['good_trial'] = False\n",
    "good_trial_idx = []\n",
    "for idate, date in enumerate(dates):\n",
    "    good_trial_idx_temp = []    \n",
    "    [good_trial_idx_temp.extend(np.where(np.logical_and(df['target_idx'][df['date']==date]==itarget+1, good_trial_idx1[df['date']==date]))[0][:min_trials_to_target]) for itarget in range(ntargets)]\n",
    "    good_trial_idx_mask = np.zeros(n_mctrials[idate], dtype=bool)\n",
    "    good_trial_idx_mask[good_trial_idx_temp] = True\n",
    "    # df['good_trial'][df['date']==date] = good_trial_idx_mask\n",
    "    df.loc[df['date']==date, ['good_trial']] = good_trial_idx_mask\n",
    "    # df.loc[df['te_id']==me.id, ['recording_site']] = exp_metadata['neuropixel_port1_site']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba96b3-8ac9-42c7-9a65-b548184ef461",
   "metadata": {},
   "source": [
    "## Plot behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1aa43c1-6d25-44ea-a755-0264d102c4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:51:58.376306Z",
     "start_time": "2024-08-01T18:51:58.369933Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:51:21.525070Z",
     "iopub.status.busy": "2024-09-19T05:51:21.524860Z",
     "iopub.status.idle": "2024-09-19T05:51:21.532319Z",
     "shell.execute_reply": "2024-09-19T05:51:21.531380Z",
     "shell.execute_reply.started": "2024-09-19T05:51:21.525051Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All trials\n",
    "samplerate = 1000\n",
    "go_cue_idx = np.ceil((df['go_cue_time'] - df['start_time'])*samplerate).astype(int)\n",
    "trial_end_idx = np.ceil((df['reach_end_time'] - df['start_time'])*samplerate).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c786b813-feb4-4b7a-b619-b03f93aa66c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:51:58.399947Z",
     "start_time": "2024-08-01T18:51:58.380366Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:51:21.533451Z",
     "iopub.status.busy": "2024-09-19T05:51:21.533245Z",
     "iopub.status.idle": "2024-09-19T05:51:21.590076Z",
     "shell.execute_reply": "2024-09-19T05:51:21.588955Z",
     "shell.execute_reply.started": "2024-09-19T05:51:21.533433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reach_time_list = [np.array(df['duration'][df['date']==date])[df['good_trial'][df['date']==date]] for date in dates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d590d9-722a-405a-85e7-0faaa944680f",
   "metadata": {},
   "source": [
    "# Load neuropixel spiking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cfa3c6c-bcb5-4138-9249-6acf36cd1a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:51:58.407493Z",
     "start_time": "2024-08-01T18:51:58.402074Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:51:21.591109Z",
     "iopub.status.busy": "2024-09-19T05:51:21.590916Z",
     "iopub.status.idle": "2024-09-19T05:51:21.597329Z",
     "shell.execute_reply": "2024-09-19T05:51:21.596152Z",
     "shell.execute_reply.started": "2024-09-19T05:51:21.591092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ntime = np.round((tafter_mc + tbefore_mc)/spike_bin_width_mc).astype(int)\n",
    "trial_time_axis = np.arange(-tbefore_mc, tafter_mc, spike_bin_width_mc)\n",
    "ntargets = len(np.unique(df['target_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9357a98b-cb84-48c1-9f82-4587d5c9d7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:52:18.087145Z",
     "start_time": "2024-08-01T18:51:58.410247Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:51:21.598120Z",
     "iopub.status.busy": "2024-09-19T05:51:21.597925Z",
     "iopub.status.idle": "2024-09-19T05:54:19.756909Z",
     "shell.execute_reply": "2024-09-19T05:54:19.755503Z",
     "shell.execute_reply.started": "2024-09-19T05:51:21.598102Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4722e05b9cf34a7db32cefc9f95dbffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate trials across sessions within a day\n",
    "# TODO: concatenatmc_entries if recorded at the same stim site but during different days\n",
    "start = time.time()\n",
    "spike_times = []\n",
    "unit_labels = []\n",
    "trial_times = []\n",
    "spike_segs = []\n",
    "spike_align = []\n",
    "spike_align_raster = []\n",
    "spike_labels = []\n",
    "spike_pos = []\n",
    "ks_labels = []\n",
    "recording_site = []\n",
    "implant_name = []\n",
    "df['recording_site'] = 0\n",
    "df['implant_name'] = ''\n",
    "\n",
    "for ime, me in enumerate(tqdm(mc_entries)):\n",
    "\n",
    "    # Load data from sessions recorded on the same day and combine \n",
    "    # Load data\n",
    "    exp_data, exp_metadata = aopy.data.load_preproc_exp_data(data_path_preproc, subject, me.id, me.date.date())\n",
    "    filename_mc = aopy.data.get_preprocessed_filename(subject, me.id, me.date.date(), 'ap')\n",
    "        \n",
    "    samplerate = exp_metadata['cursor_interp_samplerate']    \n",
    "    \n",
    "    df.loc[df['te_id']==me.id, ['recording_site']] = exp_metadata['neuropixel_port1_site']\n",
    "    df.loc[df['te_id']==me.id, ['implant_name']] = exp_metadata['neuropixel_port1_drive_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32e888ac-d214-444b-9897-964c32a99eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:52:19.471827Z",
     "start_time": "2024-08-01T18:52:18.092601Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:54:19.758379Z",
     "iopub.status.busy": "2024-09-19T05:54:19.758156Z",
     "iopub.status.idle": "2024-09-19T05:54:23.927125Z",
     "shell.execute_reply": "2024-09-19T05:54:23.926250Z",
     "shell.execute_reply.started": "2024-09-19T05:54:19.758358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get spike_seg idx for relevant events (Also will need to get idx for kinematics)\n",
    "tbefore_mc_idx = int(tbefore_mc*samplerate)\n",
    "df['delay_start_kin_idx'] = np.ceil((df['delay_start_time'] - df['start_time'])*samplerate)\n",
    "df['delay_start_neural_idx'] = np.ceil((df['delay_start_time'] - df['start_time'])*(1/spike_bin_width_mc))\n",
    "df['go_cue_kin_idx'] = np.ceil((df['go_cue_time'] - df['start_time'])*samplerate)\n",
    "df['go_cue_neural_idx'] = np.ceil((df['go_cue_time'] - df['start_time'])*(1/spike_bin_width_mc))\n",
    "df['reach_end_kin_idx'] = np.ceil((df['reach_end_time'] - df['start_time'])*samplerate)\n",
    "df['reach_end_neural_idx'] = np.ceil((df['reach_end_time'] - df['start_time'])*(1/spike_bin_width_mc))\n",
    "df['mov_onset_kin_idx'] = 0\n",
    "df['mov_onset_neural_idx'] = 0\n",
    "\n",
    "# Calculate movement onset\n",
    "# for ite in np.unique(df['te_id']): # Each TE has the same target radius\n",
    "#     # Must start at delay_start_time \n",
    "#     traj = [temp_traj[np.array(go_cue_idx[df['te_id']==ite])[itraj]:,:] for itraj, temp_traj in enumerate(df['cursor_traj'][df['te_id']==ite])]\n",
    "#     df.loc[df['te_id']==ite, ['mov_onset_kin_idx']] = np.array(get_cursor_leave_center_idx(traj, np.array(df['target_radius'][df['te_id']==ite])[0])) + np.array(go_cue_idx[df['te_id']==ite])\n",
    "#     kin_neural_samplerate_ratio = samplerate/(1/spike_bin_width_mc)\n",
    "#     df.loc[df['te_id']==ite, ['mov_onset_neural_idx']] = np.array(df['mov_onset_kin_idx'][df['te_id']==ite])//kin_neural_samplerate_ratio\n",
    "    \n",
    "# Calculate movement onset\n",
    "for ite in np.unique(df['te_id']): # Each TE has the same target radius\n",
    "    # Must start at delay_start_time \n",
    "    # traj = [temp_traj[np.array(go_cue_idx[df['te_id']==ite])[itraj]:,:] for itraj, temp_traj in enumerate(df['cursor_traj'][df['te_id']==ite])]\n",
    "    traj = [temp_traj[tbefore_mc_idx:,:] for temp_traj in df['cursor_traj'][df['te_id']==ite]]\n",
    "    movement_onset_time = aopy.analysis.behavior.get_movement_onset(np.array(traj, dtype=object), samplerate, np.array(df['hold_start_time'][df['te_id']==ite]),  np.array(df['delay_start_time'][df['te_id']==ite]), np.array(df['go_cue_time'][df['te_id']==ite]))\n",
    "    \n",
    "    df.loc[df['te_id']==ite, ['mov_onset_kin_idx']] = (movement_onset_time * samplerate).astype(int) + tbefore_mc_idx\n",
    "    kin_neural_samplerate_ratio = samplerate/(1/spike_bin_width_mc)\n",
    "    df.loc[df['te_id']==ite, ['mov_onset_neural_idx']] = np.array(df['mov_onset_kin_idx'][df['te_id']==ite])//kin_neural_samplerate_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50826ac-c1bc-40d2-beef-329d95400719",
   "metadata": {},
   "source": [
    "# Load neuropixel spike band power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cdcc724-7775-4b39-b152-177d407e8acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T05:54:23.928124Z",
     "iopub.status.busy": "2024-09-19T05:54:23.927959Z",
     "iopub.status.idle": "2024-09-19T05:54:23.933159Z",
     "shell.execute_reply": "2024-09-19T05:54:23.932328Z",
     "shell.execute_reply.started": "2024-09-19T05:54:23.928104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 [datetime.date(2023, 11, 28) datetime.date(2024, 5, 30)\n",
      " datetime.date(2024, 5, 31) datetime.date(2024, 6, 2)\n",
      " datetime.date(2024, 6, 3) datetime.date(2024, 6, 4)\n",
      " datetime.date(2024, 6, 5) datetime.date(2024, 6, 6)\n",
      " datetime.date(2024, 6, 7) datetime.date(2024, 8, 28)\n",
      " datetime.date(2024, 8, 29) datetime.date(2024, 8, 30)\n",
      " datetime.date(2024, 9, 2) datetime.date(2024, 9, 3)\n",
      " datetime.date(2024, 9, 4) datetime.date(2024, 9, 5)\n",
      " datetime.date(2024, 9, 6) datetime.date(2024, 9, 7)\n",
      " datetime.date(2024, 9, 9) datetime.date(2024, 9, 10)\n",
      " datetime.date(2024, 9, 11) datetime.date(2024, 9, 12)\n",
      " datetime.date(2024, 9, 14) datetime.date(2024, 9, 15)\n",
      " datetime.date(2024, 9, 17)]\n"
     ]
    }
   ],
   "source": [
    "print(len(dates), dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df61627b-7298-4166-bd1c-3f78fc5d565f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:52:19.484881Z",
     "start_time": "2024-08-01T18:52:19.475111Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:54:23.933847Z",
     "iopub.status.busy": "2024-09-19T05:54:23.933712Z",
     "iopub.status.idle": "2024-09-19T05:54:23.942189Z",
     "shell.execute_reply": "2024-09-19T05:54:23.941378Z",
     "shell.execute_reply.started": "2024-09-19T05:54:23.933835Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if subject == 'beignet':\n",
    "    df['recording_site'][(df['recording_site']==55)*(df['date']==dates[2])] = 56\n",
    "    df['recording_site'][(df['recording_site']==55)*(df['date']==dates[4])] = 47\n",
    "    ks_combined_tes = {dates[0]: [9922, 9924, 9925],     # (2023, 7, 13)\n",
    "                       dates[1]: [9928, 9929],           # (2023, 7, 14)\n",
    "                       dates[2]: [9940],                 # (2023, 7, 18)\n",
    "                       dates[3]: [9958],                 # (2023, 7, 19)\n",
    "                       dates[4]: [10799, 10800, 10802],  # (2023, 8, 28)\n",
    "                       dates[5]: [10810, 10812],         # (2023, 8, 29)\n",
    "                       dates[6]: [10818, 10820],         # (2023, 8, 30)\n",
    "                       dates[7]: [10824, 10827, 10828],  # (2023, 8, 31)\n",
    "                       dates[8]: [10835],                # (2023, 9, 1)\n",
    "                       dates[9]: [12269, 12270],         # (2023, 11, 16)\n",
    "                       dates[10]: [13122],               # (2023, 12, 28)\n",
    "                       dates[11]: [13239],               # (2023, 1, 3)\n",
    "                       dates[12]: [13256],               # (2023, 1, 4)\n",
    "                       dates[13]: [14116],               # (2023, 2, 1)\n",
    "                       dates[14]: [14139, 14141]}        # (2023, 2, 2)\n",
    "    \n",
    "if subject == 'affi':\n",
    "    ks_combined_tes = {dates[0]: [12386],                 # (2023, 11, 28)\n",
    "                       dates[1]: [17536],                 # (2024, 5, 30)\n",
    "                       dates[2]: [17542, 17543],                 # (2024, 5, 31)\n",
    "                       dates[3]: [17553],         # (2024, 6, 2)\n",
    "                       dates[4]: [17556],         # (2024, 6, 3)\n",
    "                       dates[5]: [17560],  # (2024, 6, 4)\n",
    "                       dates[6]: [17568],                # (2024, 6, 5)\n",
    "                       dates[7]: [17571],         # (2024, 6, 6)\n",
    "                       dates[8]: [17574], # (2024, 6, 7)\n",
    "                       dates[9]: [18110], # (2023, 8, 28)\n",
    "                       dates[10]: [18128], # (2023, 8, 29)\n",
    "                       dates[11]: [18136], # (2023, 8, 30)\n",
    "                       dates[12]: [18166], # (2023, 9, 2)\n",
    "                       dates[13]: [18170], # (2023, 9, 3)\n",
    "                       dates[14]: [18189], # (2023, 9, 4)\n",
    "                       dates[15]: [18192], # (2023, 9, 5)\n",
    "                       dates[16]: [18197], # (2024, 9, 6)\n",
    "                       dates[17]: [18199], # (2024, 9, 7)\n",
    "                       dates[18]: [18205], # (2024, 9, 9)\n",
    "                       dates[19]: [18223], # (2024, 9, 10)\n",
    "                       dates[20]: [18229], # (2024, 9, 11)\n",
    "                       dates[21]: [18236], # (2024, 9, 12)\n",
    "                       dates[22]: [18272], # (2024, 9, 14)\n",
    "                       dates[23]: [18274], # (2024, 9, 15)\n",
    "                       dates[24]: [18291], # (2024, 9, 17)\n",
    "                      }       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d7cce9-7f94-441d-80a1-ab2443b5511a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T05:54:23.942842Z",
     "iopub.status.busy": "2024-09-19T05:54:23.942702Z",
     "iopub.status.idle": "2024-09-19T05:54:23.947542Z",
     "shell.execute_reply": "2024-09-19T05:54:23.946842Z",
     "shell.execute_reply.started": "2024-09-19T05:54:23.942830Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18291]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_combined_tes[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0003ea5-3e37-41a2-a0ee-74cf0afe4b3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T19:03:41.004340Z",
     "start_time": "2024-08-01T19:02:44.743356Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T05:54:23.948219Z",
     "iopub.status.busy": "2024-09-19T05:54:23.948076Z",
     "iopub.status.idle": "2024-09-19T06:55:56.898286Z",
     "shell.execute_reply": "2024-09-19T06:55:56.897091Z",
     "shell.execute_reply.started": "2024-09-19T05:54:23.948207Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 2.5 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd77fcb5c254b4193c5e1fa24e7add4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ap band power data already exists from te17542\n",
      "Ap band power data already exists from te17543\n",
      "AP band power already exists for all TEs on 2024-05-31\n",
      "Ap band power data already exists from te17553\n",
      "AP band power already exists for all TEs on 2024-06-02\n",
      "Ap band power data already exists from te17556\n",
      "AP band power already exists for all TEs on 2024-06-03\n",
      "Ap band power data already exists from te17560\n",
      "AP band power already exists for all TEs on 2024-06-04\n",
      "Ap band power data already exists from te17568\n",
      "AP band power already exists for all TEs on 2024-06-05\n",
      "Ap band power data already exists from te17571\n",
      "AP band power already exists for all TEs on 2024-06-06\n",
      "Ap band power data already exists from te17574\n",
      "AP band power already exists for all TEs on 2024-06-07\n",
      "Ap band power data already exists from te18110\n",
      "AP band power already exists for all TEs on 2024-08-28\n",
      "Ap band power data already exists from te18128\n",
      "AP band power already exists for all TEs on 2024-08-29\n",
      "Ap band power data already exists from te18136\n",
      "AP band power already exists for all TEs on 2024-08-30\n",
      "Ap band power data already exists from te18166\n",
      "AP band power already exists for all TEs on 2024-09-02\n",
      "Ap band power data already exists from te18170\n",
      "AP band power already exists for all TEs on 2024-09-03\n",
      "Ap band power data already exists from te18189\n",
      "AP band power already exists for all TEs on 2024-09-04\n",
      "Ap band power data already exists from te18192\n",
      "AP band power already exists for all TEs on 2024-09-05\n",
      "Ap band power data already exists from te18197\n",
      "AP band power already exists for all TEs on 2024-09-06\n",
      "Ap band power data already exists from te18199\n",
      "AP band power already exists for all TEs on 2024-09-07\n",
      "Ap band power data already exists from te18205\n",
      "AP band power already exists for all TEs on 2024-09-09\n",
      "Ap band power data already exists from te18223\n",
      "AP band power already exists for all TEs on 2024-09-10\n",
      "Ap band power data already exists from te18229\n",
      "AP band power already exists for all TEs on 2024-09-11\n",
      "Ap band power data already exists from te18236\n",
      "AP band power already exists for all TEs on 2024-09-12\n",
      "Ap band power data already exists from te18272\n",
      "AP band power already exists for all TEs on 2024-09-14\n",
      "Ap band power data already exists from te18274\n",
      "AP band power already exists for all TEs on 2024-09-15\n",
      "No ap band power data found for te18291 - processing now....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6a3e1fac8a4361b606d0babd00fcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ap band power data for te18291\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "dtype = 'int16'\n",
    "nch = 384\n",
    "samplerate_dwns = 10000\n",
    "bands = [(300,3000)]\n",
    "n = .01\n",
    "w = 250\n",
    "step_size = 0.005\n",
    "ks_folder_name_cutoff = date(2023, 8, 15)\n",
    "ks_folder_name_cutoff2 = date(2024, 8, 15)\n",
    "n,p,k = aopy.precondition.base.convert_taper_parameters(n,w)\n",
    "print(n,p,k)\n",
    "\n",
    "aopy.utils.release_memory_limit()\n",
    "apband_spec_segs = []\n",
    "for idate, date in enumerate(tqdm(dates[2:])):\n",
    "    if date > ks_folder_name_cutoff and date < ks_folder_name_cutoff2:\n",
    "        ks_preproc_path = os.path.join(data_path_preproc, f\"kilosort/{date}_Neuropixel_ks_{subject}_site{list(df['recording_site'][df['date']==date])[0]}_bottom_port1\")\n",
    "    elif date > ks_folder_name_cutoff2:\n",
    "        ks_preproc_path = os.path.join(data_path_preproc, f\"kilosort/{date}_Neuropixel_{subject}_te{np.unique(df['te_id'][df['date']==date])[0]}/port1/kilosort4/\")\n",
    "    else:\n",
    "        ks_preproc_path = os.path.join(data_path_preproc, f\"kilosort/{date}_Neuropixel_ks_{subject}_bottom_port1\")\n",
    "    \n",
    "    # If all data is loaded from this day, go to next day\n",
    "    nte_id = np.unique(df['te_id'][df['date']==date])\n",
    "    try:\n",
    "        for ite_id, te_id_temp in enumerate(nte_id):\n",
    "            _ = aopy.data.base.pkl_read(f'ap_band_power_te{te_id_temp}', ap_band_power_save_dir)\n",
    "            # apband_spec_segs.extend(apband_spec_segs_teid)\n",
    "            print(f\"Ap band power data already exists from te{te_id_temp}\")\n",
    "\n",
    "        print(f\"AP band power already exists for all TEs on {date}\")\n",
    "    except:\n",
    "        # Get whitening matrix\n",
    "        if date < ks_folder_name_cutoff2:\n",
    "            rez_path = os.path.join(ks_preproc_path, 'kilosort_output/rez.mat')\n",
    "            with h5py.File(rez_path, 'r') as f:\n",
    "                whitening_matrix = np.array(f['rez']['Wrot'])\n",
    "        else:\n",
    "            rez_path = os.path.join(ks_preproc_path, 'ops.npy')\n",
    "            whitening_matrix = np.load(rez_path, allow_pickle=True).item()['Wrot']\n",
    "\n",
    "        inv_wht_matrix = np.linalg.pinv(whitening_matrix)\n",
    "\n",
    "        # Load drift corrected and whitened apdata\n",
    "        drift_corrected_apdata_path = os.path.join(ks_preproc_path, 'temp_wh.dat')\n",
    "        data = np.memmap(drift_corrected_apdata_path, mode='r', dtype=dtype).reshape(-1,nch)\n",
    "\n",
    "        # Correct by multiplying ap data by inverse whitening matrix\n",
    "        corrected_data = data @ inv_wht_matrix\n",
    "        # corrected_data = data\n",
    "\n",
    "        \n",
    "        # Load raw apdata and check that it is the same length. If not, subselect the relevant data. This happens because data was concatenated before kilosort    \n",
    "        temp_data = []\n",
    "        if len(ks_combined_tes[date]) > 1:\n",
    "            relevant_tes = np.unique(df['te_id'][df['date']==date])\n",
    "            start_sample = 0\n",
    "            for te_id_temp in ks_combined_tes[date]:\n",
    "                data_folder_mc = f\"{date}_Neuropixel_{subject}_te{te_id_temp}\"\n",
    "                rawdata_mc, rawmetadata = aopy.data.neuropixel.load_neuropixel_data(data_path_raw, data_folder_mc, 'ap')\n",
    "                end_sample = start_sample + rawdata_mc.samples.shape[0]\n",
    "                if te_id_temp in relevant_tes:\n",
    "                    temp_data.append(corrected_data[start_sample:end_sample,:])\n",
    "                start_sample = end_sample\n",
    "\n",
    "        else:\n",
    "            temp_data = [corrected_data]   \n",
    "            \n",
    "        del corrected_data, data # Clear up memory\n",
    "            # print(f\"WARNING: Mismatch of samples between temp_wh.dat file and raw ap data -- please check\")\n",
    "        \n",
    "        # Downsample AP band data --- this needs to handle multiple TEs\n",
    "        for ite_id, te_id_temp in enumerate(nte_id):\n",
    "            try:\n",
    "                apband_spec_segs_teid = aopy.data.base.pkl_read(f'ap_band_power_te{te_id_temp}', ap_band_power_save_dir)\n",
    "                # apband_spec_segs.extend(apband_spec_segs_teid)\n",
    "                print(f\"Ap band power data already exists from te{te_id_temp}\")\n",
    "            except:\n",
    "                print(f\"No ap band power data found for te{te_id_temp} - processing now....\")\n",
    "                \n",
    "                preproc_filename_mc = aopy.data.get_preprocessed_filename(subject, te_id_temp, date, 'lfp')\n",
    "                if date > ks_folder_name_cutoff2:\n",
    "                    preproc_filename_mc = preproc_filename_mc[:-4] + '_port1.hdf'\n",
    "                    \n",
    "                lfp_data_mc = aopy.data.load_hdf_group(os.path.join(data_path_preproc, subject), preproc_filename_mc, 'lfp')\n",
    "                lfp_metadata_mc = aopy.data.load_hdf_group(os.path.join(data_path_preproc, subject), preproc_filename_mc, 'metadata')\n",
    "                \n",
    "                data_folder_mc = f\"{date}_Neuropixel_{subject}_te{te_id_temp}\"\n",
    "                rawdata_mc, metadata_mc = aopy.data.neuropixel.load_neuropixel_data(data_path_raw, data_folder_mc, 'ap')\n",
    "                corrected_apdata_dwns = aopy.precondition.base.downsample(temp_data[ite_id], metadata_mc['sample_rate'], samplerate_dwns)\n",
    "                ap_timestamps,_ = aopy.preproc.base.interp_timestamps2timeseries(lfp_data_mc['sync_timestamp'], lfp_data_mc['sync_timestamp'], samplerate=samplerate_dwns) # upsample lfp timestamps to AP\n",
    "\n",
    "                # Trial align ap_timestamps for each TE and put into DF\n",
    "                traj_times = np.array([(hst-tbefore_mc, e[-1]+tafter_mc) for hst, e in zip(np.array(df['hold_start_time'][df['te_id']==te_id_temp]), np.array(df['event_times'][df['te_id']==te_id_temp]))])\n",
    "                ntrials_teid=traj_times.shape[0]\n",
    "                talign_times_mc_segs = []\n",
    "                talign_idx_mc_segs = []\n",
    "                apband_spec_segs_teid = []\n",
    "                for itrial in tqdm(range(ntrials_teid)):\n",
    "                    talign_times_mc, talign_idx_mc = aopy.preproc.base.trial_align_times(ap_timestamps, [traj_times[itrial,0]], 0, traj_times[itrial,1]-traj_times[itrial,0])\n",
    "                    talign_times_mc_segs.append(talign_times_mc)\n",
    "                    talign_idx_mc_segs.append(talign_idx_mc)\n",
    "                \n",
    "                    #filter\n",
    "                    t, spec_temp = aopy.analysis.base.get_bandpower_feats(corrected_apdata_dwns[talign_idx_mc[0],:], samplerate_dwns, bands=bands, log=True, ref=True, \n",
    "                                                                n=n, p=p, k=k, fk=bands[0][1], step = step_size)\n",
    "                    apband_spec_segs_teid.append(aopy.precondition.base.bin_spikes(spec_temp, int(1/step_size), spike_bin_width_mc))\n",
    "                # apband_spec_segs.extend(apband_spec_segs_teid)\n",
    "                \n",
    "                ap_band_power_metadata = {'samplerate': int(1/spike_bin_width_mc), 'frequency_band': bands, 'ch_ypos': lfp_metadata_mc['ypos'], 'ch_xpos': lfp_metadata_mc['xpos']}\n",
    "\n",
    "                if save_ap_band_power:\n",
    "                    print(f\"Saving ap band power data for te{te_id_temp}\")\n",
    "                    aopy.data.base.pkl_write(f'ap_band_power_te{te_id_temp}', (apband_spec_segs_teid, ap_band_power_metadata), ap_band_power_save_dir)\n",
    "                    print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872e79fa-5844-4cab-bf85-19b4c061c759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T06:55:56.899452Z",
     "iopub.status.busy": "2024-09-19T06:55:56.899285Z",
     "iopub.status.idle": "2024-09-19T06:55:56.906243Z",
     "shell.execute_reply": "2024-09-19T06:55:56.905222Z",
     "shell.execute_reply.started": "2024-09-19T06:55:56.899437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18291]\n"
     ]
    }
   ],
   "source": [
    "print(nte_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228e2e2a-e811-486b-b6a9-78b4f914b46f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T06:55:56.906781Z",
     "iopub.status.busy": "2024-09-19T06:55:56.906657Z",
     "iopub.status.idle": "2024-09-19T06:55:56.913089Z",
     "shell.execute_reply": "2024-09-19T06:55:56.912085Z",
     "shell.execute_reply.started": "2024-09-19T06:55:56.906770Z"
    }
   },
   "outputs": [],
   "source": [
    "# def preproc_ap_band_data(ks_preproc_path, samplerate_original=30000, samplerate_dwns=10000):\n",
    "#         # Get whitening matrix\n",
    "#         rez_path = os.path.join(ks_preproc_path, 'kilosort_output/rez.mat')\n",
    "#         with h5py.File(rez_path, 'r') as f:\n",
    "#             whitening_matrix = np.array(f['rez']['Wrot'])\n",
    "\n",
    "#         inv_wht_matrix = np.linalg.pinv(whitening_matrix)\n",
    "\n",
    "#         # Load drift corrected and whitened apdata\n",
    "#         drift_corrected_apdata_path = os.path.join(ks_preproc_path, 'temp_wh.dat')\n",
    "#         data = np.memmap(drift_corrected_apdata_path, mode='r', dtype=dtype).reshape(-1,nch)\n",
    "\n",
    "#         # Correct by multiplying ap data by inverse whitening matrix\n",
    "#         corrected_data = data @ inv_wht_matrix\n",
    "\n",
    "#         # Load raw apdata and check that it is the same length. If not, subselect the relevant data. This happens because data was concatenated before kilosort    \n",
    "#         temp_data = []\n",
    "#         if len(ks_combined_tes[date]) > 1:\n",
    "#             relevant_tes = np.unique(df['te_id'][df['date']==date])\n",
    "#             start_sample = 0\n",
    "#             for te_id_temp in ks_combined_tes[date]:\n",
    "#                 data_folder_mc = f\"{date}_Neuropixel_{subject}_te{te_id_temp}\"\n",
    "#                 rawdata_mc, rawmetadata = aopy.data.neuropixel.load_neuropixel_data(data_path_raw, data_folder_mc, 'ap')\n",
    "#                 end_sample = start_sample + rawdata_mc.samples.shape[0]\n",
    "#                 if te_id_temp in relevant_tes:\n",
    "#                     temp_data.append(corrected_data[start_sample:end_sample,:])\n",
    "#                 start_sample = end_sample\n",
    "\n",
    "#         else:\n",
    "#             temp_data = [corrected_data]   \n",
    "            \n",
    "#         del corrected_data, data # Clear up memory\n",
    " \n",
    "#         # Downsample AP band data --- this needs to handle multiple TEs for concatenated datasets\n",
    "#         for ite_id, te_id_temp in enumerate(nte_id):\n",
    "#             try: # Should switch this to a more direct test of if the file exists\n",
    "#                 apband_spec_segs_teid = aopy.data.base.pkl_read(f'ap_band_power_te{te_id_temp}', ap_band_power_save_dir)\n",
    "#                 print(f\"Ap band power data already exists from te{te_id_temp}\")\n",
    "#             except:\n",
    "#                 print(f\"No ap band power data found for te{te_id_temp} - processing now....\")\n",
    "#                 preproc_filename_mc = aopy.data.get_preprocessed_filename(subject, te_id_temp, date, 'lfp')\n",
    "#                 lfp_data_mc = aopy.data.load_hdf_group(os.path.join(data_path_preproc, subject), preproc_filename_mc, 'lfp')\n",
    "#                 lfp_metadata_mc = aopy.data.load_hdf_group(os.path.join(data_path_preproc, subject), preproc_filename_mc, 'metadata')\n",
    "                \n",
    "#                 data_folder_mc = f\"{date}_Neuropixel_{subject}_te{te_id_temp}\"\n",
    "#                 rawdata_mc, metadata_mc = aopy.data.neuropixel.load_neuropixel_data(data_path_raw, data_folder_mc, 'ap')\n",
    "#                 corrected_apdata_dwns = aopy.precondition.base.downsample(temp_data[ite_id], samplerate_original, samplerate_dwns)\n",
    "#                 ap_timestamps,_ = aopy.preproc.base.interp_timestamps2timeseries(lfp_data_mc['sync_timestamp'], lfp_data_mc['sync_timestamp'], samplerate=samplerate_dwns) # upsample lfp timestamps to AP\n",
    "\n",
    "#         return corrected_apdata_dwns, ap_timestamps, samplerate_dwns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81df7f-b6a2-47cc-afda-457b662c4a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8b083-6318-4758-9b9f-f4045f22f92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:np_targeting]",
   "language": "python",
   "name": "conda-env-np_targeting-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
