{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d446e86-44e7-4d8a-8212-5a22da47136f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T00:03:52.219938Z",
     "iopub.status.busy": "2025-01-31T00:03:52.219471Z",
     "iopub.status.idle": "2025-01-31T00:03:52.244408Z",
     "shell.execute_reply": "2025-01-31T00:03:52.243143Z",
     "shell.execute_reply.started": "2025-01-31T00:03:52.219885Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_hdf_group(data_dir, hdf_filename, group=\"/\"):\n",
    "    '''\n",
    "    Loads any datasets from the given hdf group into a dictionary. Also will\n",
    "    recursively load other groups if any exist under the given group\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): folder where data is located\n",
    "        hdf_filename (str): name of hdf file\n",
    "        group (str): name of the group to load\n",
    "    \n",
    "    Returns:\n",
    "        dict: all the datasets contained in the given group\n",
    "    '''\n",
    "    full_file_name = os.path.join(data_dir, hdf_filename)\n",
    "    hdf = h5py.File(full_file_name, 'r')\n",
    "    if group not in hdf:\n",
    "        raise ValueError('No such group in file {}'.format(hdf_filename))\n",
    "\n",
    "    # Recursively load groups until datasets are reached\n",
    "    def _load_hdf_group(hdf):\n",
    "        keys = hdf.keys()\n",
    "        data = dict()\n",
    "        for k in keys:\n",
    "            if isinstance(hdf[k], h5py.Group):\n",
    "                data[k] = _load_hdf_group(hdf[k])\n",
    "            else:\n",
    "                k_, v = _load_hdf_dataset(hdf[k], k)\n",
    "                data[k_] = v\n",
    "        return data\n",
    "\n",
    "    data = _load_hdf_group(hdf[group])\n",
    "    hdf.close()\n",
    "    return data\n",
    "\n",
    "def _load_hdf_dataset(dataset, name):\n",
    "    '''\n",
    "    Internal function for loading hdf datasets. Decodes json and unicode data automatically.\n",
    "\n",
    "    Args:\n",
    "        dataset (hdf object): dataset to load\n",
    "        name (str): name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing:\n",
    "            | **name (str):** name of the dataset (might be modified)\n",
    "            | **data (object):** loaded data\n",
    "    '''\n",
    "    data = dataset[()]\n",
    "    if '_json' in name:\n",
    "        import json\n",
    "        name = name.replace('_json', '')\n",
    "        data = json.loads(data)\n",
    "    try:\n",
    "        data = data.decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    return name, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5802cedd-b3cd-4c74-b71e-a49db1d6c29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T00:03:52.247006Z",
     "iopub.status.busy": "2025-01-31T00:03:52.246715Z",
     "iopub.status.idle": "2025-01-31T00:03:55.894762Z",
     "shell.execute_reply": "2025-01-31T00:03:55.892766Z",
     "shell.execute_reply.started": "2025-01-31T00:03:52.246978Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aolab/miniconda3/envs/np_targeting/lib/python3.9/site-packages/one/alf/files.py:10: FutureWarning: `one.alf.files` will be removed in version 3.0. Use `one.alf.path` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import aopy\n",
    "import os\n",
    "import pandas as pds\n",
    "from db import dbfunctions as db\n",
    "from ipywidgets import interactive, widgets\n",
    "import scipy\n",
    "import h5py\n",
    "from tqdm.auto import tqdm \n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from itertools import compress\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import math\n",
    "from scipy.fft import fft\n",
    "import glob\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b1345-3532-4c0a-be1b-9895db46adda",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6e7969-b9c3-4121-8594-a6646a7497f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T00:03:55.899910Z",
     "iopub.status.busy": "2025-01-31T00:03:55.899600Z",
     "iopub.status.idle": "2025-01-31T00:03:55.906326Z",
     "shell.execute_reply": "2025-01-31T00:03:55.905064Z",
     "shell.execute_reply.started": "2025-01-31T00:03:55.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "base_save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/\"\n",
    "np_preproc_data_folder = 'np_analysis_preproc_data'\n",
    "ecog_dec_acc_file_name = 'ecog_decoding_maps/npinsert_ecog_decoding'\n",
    "\n",
    "subject = 'affi'\n",
    "align_events = ['TARGET ONSET', 'GO CUE', 'MOVEMENT ONSET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd692701-dc60-4b11-a361-5a93fc22ada2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T00:03:55.907236Z",
     "iopub.status.busy": "2025-01-31T00:03:55.906935Z",
     "iopub.status.idle": "2025-01-31T00:03:55.915125Z",
     "shell.execute_reply": "2025-01-31T00:03:55.913832Z",
     "shell.execute_reply.started": "2025-01-31T00:03:55.907205Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decoding calculation parameters\n",
    "tbefore = 0.5\n",
    "tafter = 1\n",
    "nlda_lags = 1\n",
    "niter_match = 50\n",
    "min_trial_prop = .85\n",
    "ntrial_bin_size = 96\n",
    "nfolds = 4\n",
    "\n",
    "# Visualization parameters\n",
    "colors = sn.color_palette(n_colors=9)\n",
    "recording_brain_areas={'M1': [30, 56, 47, 40, 121, 48, 120, 98], 'PM':[11, 9, 18, 22, 10, 45]}\n",
    "day_colors = ['dodgerblue', 'indigo', 'violet', 'lightblue', 'mediumorchid',\n",
    "              'purple', 'steelblue', 'dodgerblue', 'lightblue', 'red', 'black', 'green', 'purple', 'cyan', 'gray', 'yellow'] \n",
    "\n",
    "save_dir = \"/media/moor-data/results/Ryan/neuropixel_targeting/np_analysis_preproc_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00433fce-e97d-46aa-b9c7-c2d30cd2998a",
   "metadata": {},
   "source": [
    "# Load and extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4381962-bc14-4232-8991-643780d7c1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T00:03:55.916435Z",
     "iopub.status.busy": "2025-01-31T00:03:55.916128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "aopy.utils.release_memory_limit()\n",
    "df, rasters, preproc_metadata = aopy.data.base.pkl_read(f\"{subject}_np_preprocessed\", os.path.join(base_save_dir, np_preproc_data_folder))\n",
    "print(f\"{np.round((time.time()-start)/60)} min to load preprocessed data\")\n",
    "nrecs = preproc_metadata['nrecs']\n",
    "recording_site = preproc_metadata['recording_sites'] # will be the same for all align events\n",
    "implants = ['NP_Insert72' if preproc_metadata['implant'][irec] == 'NP_Insert72' else 'NP_Insert137' for irec in range(len(preproc_metadata['implant']))] #Rename because name in bmi3d is slightly different (TODO)\n",
    "dates = np.unique(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099da1e-5d21-40dc-b084-7da935c18dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecog_dec_acc = load_hdf_group(base_save_dir, ecog_dec_acc_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c58de2-234d-416e-a7ae-63d0d7357120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qc_results, ksdrift = aopy.data.base.pkl_read(f\"{subject}_QCunits\", os.path.join(base_save_dir, np_preproc_data_folder))\n",
    "# stable_unit_labels = [qc_results['final_good_unit_labels'][irec] for irec in range(nrecs)]\n",
    "# stable_unit_idx = [qc_results['final_good_unit_idx'][irec] for irec in range(nrecs)]\n",
    "# nstable_unit = np.array([len(qc_results['final_good_unit_idx'][irec]) for irec in range(nrecs)])\n",
    "# neuron_pos = [qc_results['position'][irec] for irec in range(nrecs)]\n",
    "\n",
    "# if subject == 'affi':\n",
    "stable_unit_labels = [qc_results['manual_good_unit_labels'][irec] for irec in range(nrecs)]\n",
    "stable_unit_idx = [qc_results['manual_good_unit_idx'][irec] for irec in range(nrecs)]\n",
    "nstable_unit = np.array([len(qc_results['manual_good_unit_idx'][irec]) for irec in range(nrecs)])\n",
    "neuron_pos = [qc_results['manual_position'][irec] for irec in range(nrecs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b4fd6-34ba-403d-964a-8a3f0d801a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if subject == 'beignet':\n",
    "    surface_pos = np.ones(nrecs)*3840\n",
    "elif subject == 'affi':\n",
    "    surface_pos = np.array([3840, 3840, 3400, 3250,3840,3800,3800,3900,3500,3700,2500,3250,2100,3840,3100,1600,3250,2500,3100,3300,3800,3100,3250,2750,2900, 2750,\n",
    "                           3000,3000, 3000,3700, 3000,3700, 3000,3700, 35000])\n",
    "print(preproc_metadata['recording_sites'])\n",
    "print(surface_pos.shape, nrecs, surface_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2c0de-8ba7-41a5-bf6c-de5b682bec77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile dataframe of unit information\n",
    "# Need columns for: date, recording site, unit_label, absolute y-pos, relative y-pos, waveform\n",
    "unit_info = {'rec_number': [], 'date': [], 'rec_site': [], 'rec_xpos': [], 'rec_ypos': [], 'rec_rcaxis': [], 'rec_mlaxis': [], 'implant': [],\n",
    "             'unit_label': [], 'unit_idx': [], 'abs_depth': [], 'rel_depth': [], 'waveform': [], 'surface_pos': [], 'penetration': []}\n",
    "# unit_info = {'rec_number': [], 'date': [], 'rec_site': [], 'rec_xpos': [], 'rec_ypos': [], 'implant': [],\n",
    "#              'unit_label': [], 'unit_idx': []}\n",
    "surface_buffer = 200\n",
    "for irec in tqdm(range(nrecs)):\n",
    "    # print(irec, nstable_unit[irec], neuron_pos[irec].shape)\n",
    "    for iunit in range(nstable_unit[irec]):\n",
    "        # Only save units that are below the estimated surface of the brain\n",
    "        # if neuron_pos[irec][iunit] <= (surface_pos[irec]+surface_buffer):   \n",
    "        \n",
    "        unit_info['rec_number'].append(irec) # Recording_number\n",
    "        unit_info['date'].append(list(df[df['penetration']==irec]['date'])[0]) # date (assumes each recording is done on a different day)\n",
    "        unit_info['rec_site'].append(preproc_metadata['recording_sites'][irec]) # Recording site\n",
    "        unit_info['rec_xpos'].append(ecog_dec_acc[subject]['rec_locations'][irec,0]) # Recording site x pos in chamber\n",
    "        unit_info['rec_ypos'].append(ecog_dec_acc[subject]['rec_locations'][irec,1]) # Recording site y pos in chamber\n",
    "        unit_info['rec_rcaxis'].append(ecog_dec_acc[subject]['rc_axis'][irec,0]) # Recording site y pos in chamber\n",
    "        unit_info['rec_mlaxis'].append(ecog_dec_acc[subject]['rc_axis'][irec,1]) # Recording site y pos in chamber\n",
    "        unit_info['implant'].append(implants[irec]) # Implant\n",
    "        unit_info['unit_label'].append(stable_unit_labels[irec][iunit]) # unit label output from kilosort\n",
    "        unit_info['unit_idx'].append(stable_unit_idx[irec][iunit]) # unit label output from kilosort\n",
    "        unit_info['abs_depth'].append(surface_pos[irec] - neuron_pos[irec][iunit]) #Absolute depth [um]\n",
    "        unit_info['rel_depth'].append(3840-neuron_pos[irec][iunit]-np.min(3840-neuron_pos[irec])) #Relative depth [um] (to top detected neuron)\n",
    "        unit_info['waveform'].append(qc_results['mean_wfs'][irec][iunit,:]) #waveform\n",
    "        unit_info['surface_pos'].append(surface_pos[irec])\n",
    "        unit_info['penetration'].append(irec)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22c72e-b007-4bb4-9f54-b43525e6f845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unit_df = pds.DataFrame(unit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c27f6f-9304-484d-b19d-a5d1505c3f6f",
   "metadata": {},
   "source": [
    "# Create pseudopopulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ea534-93ef-412d-b436-9c2a6640b462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_configs = ['random', 'column', 'depth'] # Pseudopopulation cofigurations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f473ec7-a65c-4dc6-b02e-7a4cd8364ef3",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fbb39-0e5a-47b0-a548-d657e70b0649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create random neuron populations\n",
    "nrandom_units = 10 # Number of random units chosen in each group\n",
    "nrandom_groups = 1000 # Number of random groups to make\n",
    "random_group_info = []\n",
    "for igroup in range(nrandom_groups):\n",
    "    group_idx = np.random.choice(np.arange(len(unit_df)), size=(nrandom_units), replace=False)\n",
    "    random_group_info.append(unit_df.loc[group_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3c1a9-aff8-4f9f-a424-19a89a3c71a3",
   "metadata": {},
   "source": [
    "## Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff199eb7-dd3c-4271-8226-9a9b2ec79d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_rec_pos, unique_rec_pos_idx = np.unique(ecog_dec_acc[subject]['rec_locations'], axis=0, return_index=True)\n",
    "column_group_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f8810-8fdd-42ed-a1c1-5b6f838f3a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create random neuron populations from each recording_location\n",
    "min_probe_depth = 0 #[um]\n",
    "unique_rec_pos, unique_rec_pos_idx = np.unique(ecog_dec_acc[subject]['rec_locations'], axis=0, return_index=True)\n",
    "column_group_info = []\n",
    "for ipos in range(unique_rec_pos.shape[0]):\n",
    "    pos = unique_rec_pos[ipos,:]\n",
    "    new_df = unit_df.loc[(unit_df['rec_xpos']==pos[0]) & (unit_df['rec_ypos']==pos[1]) & (unit_df['surface_pos']>min_probe_depth)].reset_index()\n",
    "    \n",
    "    # Add a row of 0 if there are no units so code works. There is no recording site 0 because it is 1-indexed.\n",
    "    if len(new_df)==0:   \n",
    "        new_df.loc[0] = [np.nan] * len(new_df.columns)\n",
    "        new_df['rec_site'] = ecog_dec_acc[subject]['rec_sites'][unique_rec_pos_idx[ipos]]\n",
    "        new_df['implant'] = implants[unique_rec_pos_idx[ipos]]\n",
    "        \n",
    "    column_group_info.append(new_df) #only use sites whose surface position is greater than a threshold\n",
    "\n",
    "    # print(unit_df.loc[(unit_df['rec_xpos']==pos[0]) & (unit_df['rec_ypos']==pos[1])].reset_index())\n",
    "    # print(len(column_group_info[ipos]),column_group_info[ipos]['rec_site'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacf4ab-ef8d-4c9b-b707-9e916a6325c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('nUnits:  ', [len(column_group_info[ii]) for ii in range(len(column_group_info))])\n",
    "print('Rec Site:', [column_group_info[ii]['rec_site'][0] for ii in range(len(column_group_info))])\n",
    "print(len(unique_rec_pos))\n",
    "print(unique_rec_pos)\n",
    "print(recording_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663e792-6bd0-4cce-abae-d4ec194279a3",
   "metadata": {},
   "source": [
    "## Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff454c8-e8c7-482b-9004-d3c9afbb75d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# depth_ranges = [(0,1000), (1000, 2000), (2000, 100000)] #Boundary distinguishing shallow and deep neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9957bb-f500-4bef-ba38-e842a789cfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create random neuron populations from neurons at different depths\n",
    "\n",
    "depth_ranges = [(0,1000),(250,1250),(500,1500),(750,1750),(1000,2000),(1250,2250),(1500,2500),(1750,2750),(2000,3000),(2250,3250),(2500,3500),(2750,3750),(np.max(unit_df['rel_depth'])-1000,np.max(unit_df['rel_depth']))] #Boundary distinguishing shallow and deep neurons \n",
    "\n",
    "depth_group_info = []\n",
    "for irange, depth_range in enumerate(depth_ranges):\n",
    "    depth_group_info.append(unit_df.loc[(unit_df['rel_depth'] >= depth_range[0]) & (unit_df['rel_depth'] < depth_range[1])].reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef30a12-00a5-464f-a72f-8c36ded1ca89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create random neuron populations for each recording site from neurons at different depths\n",
    "\n",
    "depth_group_info_by_site = {}\n",
    "unique_sites = np.unique(unit_df['rec_site'])\n",
    "for isite, site in enumerate(unique_sites):\n",
    "    depth_group_info_by_site[site] = []\n",
    "    for irange, depth_range in enumerate(depth_ranges):\n",
    "        depth_mask = (unit_df['rel_depth'] >= depth_range[0]) & (unit_df['rel_depth'] < depth_range[1])\n",
    "        site_mask = unit_df['rec_site'] == site\n",
    "        depth_group_info_by_site[site].append(unit_df.loc[depth_mask*site_mask].reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0c97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for isite, site in enumerate(unique_sites):\n",
    "    print(site, [len(depth_group_info_by_site[site][idepth]) for idepth in range(len(depth_group_info_by_site[site]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3802b88-a10b-49c1-8c08-0c99cec6efba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(column_group_info[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dabf5",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121813b3-8d62-451f-ab69-6a31bfbb61be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pseudopopulation_metadata = {'nrandom_units': nrandom_units, 'nrandom_groups': nrandom_groups, 'depth_ranges': depth_ranges}\n",
    "print(pseudopopulation_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e880982-c856-4aac-b805-1a4d75af210c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "aopy.data.base.pkl_write(f\"{subject}_np_psuedopopulations\", (random_group_info, column_group_info, depth_group_info, depth_group_info_by_site, pseudopopulation_metadata, unit_df), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e80ed-dfa1-4132-b81a-a1666257edda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(depth_group_info[0]))\n",
    "print(len(depth_group_info[1]))\n",
    "print(len(depth_group_info[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4eb15-15d3-492c-9bce-9ab64035ccd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aopy.utils.get_memory_available_gb()\n",
    "type(df['unit_labels'][df['date']==dates[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfc1a4-4e6c-4722-b3e5-63a4489adc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5487206-0423-4f3c-babc-eeb277610fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8521630-fa5e-42f7-a701-5709d6694596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716a869-52d9-45d3-97cc-0144c383a1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aaceda-23cc-49ba-becf-3c420cdc25f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cacde-7044-4538-b5fd-11d5d32f95d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ba814-536e-42c5-94a0-a28d6e094102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:np_targeting]",
   "language": "python",
   "name": "conda-env-np_targeting-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
